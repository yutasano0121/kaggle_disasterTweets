{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from time import localtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker parameters\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'kaggle/tweetSentiment'  # Prefix should not tontain '/' at the end!\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "working_dir = '/home/ec2-user/SageMaker/kaggle_data'\n",
    "data_dir = os.path.join(working_dir, 'processed_data/')\n",
    "cache_dir = os.path.join(working_dir, 'cache/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data to train and velidation data.\n",
    "train = pd.read_csv(os.path.join(data_dir, 'train_processed.csv'), header=None)\n",
    "train, validation = train_test_split(\n",
    "    train, \n",
    "    stratify=train[0].values,\n",
    "    test_size=0.25,\n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the split data to the local.\n",
    "train.to_csv(os.path.join(data_dir, 'train_processed_split.csv'), header=None, index=None)\n",
    "validation.to_csv(os.path.join(data_dir, 'validation.csv'), header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data already present in S3.\n"
     ]
    }
   ],
   "source": [
    "# data upload to S3\n",
    "s3_object_dict = s3.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=prefix\n",
    ")\n",
    "\n",
    "# Fetch filenames in the data directory.\n",
    "local_data_list = os.listdir(data_dir)\n",
    "\n",
    "# Combine the name with the S3 bucket prefix.\n",
    "local_data_list = [os.path.join(prefix, f) for f in local_data_list]\n",
    "\n",
    "# Upload the data if they are not present in S3.\n",
    "try:\n",
    "    s3_object_list = [content['Key'] for content in s3_object_dict['Contents']]\n",
    "    if set(local_data_list).intersection(s3_object_list) == set(local_data_list):\n",
    "        train_location = os.path.join('s3://', bucket, prefix, 'train_processed_split.csv')\n",
    "        validation_location = os.path.join('s3://', bucket, prefix, 'validation.csv')\n",
    "        \n",
    "        print(\"input_data already present in S3.\")\n",
    "        \n",
    "    else:\n",
    "        # Split the training data to train and velidation data.\n",
    "        train = pd.read_csv(os.path.join(data_dir, 'train_processed.csv'), header=None)\n",
    "        train, validation = train_test_split(\n",
    "            train, \n",
    "            stratify=train[0].values,\n",
    "            test_size=0.25,\n",
    "            random_state=0\n",
    "        )\n",
    "        \n",
    "        # Save them locally.\n",
    "        train.to_csv(os.path.join(data_dir, 'train_processed_split.csv'), header=None, index=None)\n",
    "        validation.to_csv(os.path.join(data_dir, 'validation.csv'), header=None, index=None)\n",
    "        \n",
    "        train = None,\n",
    "        validation = None\n",
    "        \n",
    "        # Upload the data to S3.\n",
    "        train_location = session.upload_data(\n",
    "            path=os.path.join(data_dir, 'train_processed_split.csv'), \n",
    "            bucket=bucket,\n",
    "            key_prefix=prefix\n",
    "        )\n",
    "        validation_location = session.upload_data(\n",
    "            path=os.path.join(data_dir, 'validation.csv'), \n",
    "            bucket=bucket,\n",
    "            key_prefix=prefix\n",
    "        )\n",
    "        \n",
    "        print(\"train and validation data uploaded to S3.\")\n",
    "        \n",
    "except KeyError:  # if nothing exists in the S3 bucket.\n",
    "    # Split the training data to train and velidation data.\n",
    "    train = pd.read_csv(os.path.join(data_dir, 'train_processed.csv'), header=None)\n",
    "    train, validation = train_test_split(\n",
    "        train, \n",
    "        stratify=train[0].values,\n",
    "        test_size=0.25,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    # Save them locally.\n",
    "    train.to_csv(os.path.join(data_dir, 'train_processed_split.csv'), header=None, index=None)\n",
    "    validation.to_csv(os.path.join(data_dir, 'validation.csv'), header=None, index=None)\n",
    "\n",
    "    train = None,\n",
    "    validation = None\n",
    "    \n",
    "    # Upload the data to S3.\n",
    "    train_location = session.upload_data(\n",
    "        path=os.path.join(data_dir, 'train_processed_split.csv'), \n",
    "        bucket=bucket,\n",
    "        key_prefix=prefix\n",
    "    )\n",
    "    validation_location = session.upload_data(\n",
    "        path=os.path.join(data_dir, 'validation.csv'), \n",
    "        bucket=bucket,\n",
    "        key_prefix=prefix\n",
    "    )\n",
    "    \n",
    "    print(\"train and validation data uploaded to S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to know the name of the container that we want to use for training. SageMaker provides\n",
    "# a nice utility method to construct this for us.\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost', '0.90-2')\n",
    "\n",
    "# We now specify the parameters we wish to use for our training job\n",
    "training_params = {}\n",
    "\n",
    "# We need to specify the permissions that this training job will have. For our purposes we can use\n",
    "# the same permissions that our current SageMaker session has.\n",
    "training_params['RoleArn'] = role\n",
    "\n",
    "# Here we describe the algorithm we wish to use. The most important part is the container which\n",
    "# contains the training code.\n",
    "training_params['AlgorithmSpecification'] = {\n",
    "    \"TrainingImage\": container,\n",
    "    \"TrainingInputMode\": \"File\"\n",
    "}\n",
    "\n",
    "# We also need to say where we would like the resulting model artifacts stored.\n",
    "training_params['OutputDataConfig'] = {\n",
    "    \"S3OutputPath\": os.path.join(\"s3://\", bucket, prefix, \"output\")\n",
    "}\n",
    "\n",
    "# We also need to set some parameters for the training job itself. Namely we need to describe what sort of\n",
    "# compute instance we wish to use along with a stopping condition to handle the case that there is\n",
    "# some sort of error and the training script doesn't terminate.\n",
    "training_params['ResourceConfig'] = {\n",
    "    \"InstanceCount\": 1,\n",
    "    \"InstanceType\": \"ml.m4.xlarge\",\n",
    "    \"VolumeSizeInGB\": 5\n",
    "}\n",
    "    \n",
    "training_params['StoppingCondition'] = {\n",
    "    \"MaxRuntimeInSeconds\": 86400\n",
    "}\n",
    "\n",
    "# Next we set the algorithm specific hyperparameters. In this case, since we are setting up\n",
    "# a training job which will serve as the base training job for the eventual hyperparameter\n",
    "# tuning job, we only specify the _static_ hyperparameters. That is, the hyperparameters that\n",
    "# we do _not_ want SageMaker to change.\n",
    "training_params['StaticHyperParameters'] = {\n",
    "    \"gamma\": \"4\",\n",
    "    \"subsample\": \"0.8\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"early_stopping_rounds\": \"100\",\n",
    "    \"num_round\": \"1000\"\n",
    "}\n",
    "\n",
    "# Now we need to tell SageMaker where the data should be retrieved from.\n",
    "training_params['InputDataConfig'] = [\n",
    "    {\n",
    "        \"ChannelName\": \"train\",\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": train_location,\n",
    "                \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "            }\n",
    "        },\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    },\n",
    "    {\n",
    "        \"ChannelName\": \"validation\",\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": validation_location,\n",
    "                \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "            }\n",
    "        },\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to construct a dictionary which specifies the tuning job we want SageMaker to perform\n",
    "tuning_job_config = {\n",
    "    # First we specify which hyperparameters we want SageMaker to be able to vary,\n",
    "    # and we specify the type and range of the hyperparameters.\n",
    "    \"ParameterRanges\": {\n",
    "    \"CategoricalParameterRanges\": [],\n",
    "    \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "            \"MaxValue\": \"0.5\",\n",
    "            \"MinValue\": \"0.05\",\n",
    "            \"Name\": \"eta\"\n",
    "        },\n",
    "    ],\n",
    "    \"IntegerParameterRanges\": [\n",
    "        {\n",
    "            \"MaxValue\": \"12\",\n",
    "            \"MinValue\": \"3\",\n",
    "            \"Name\": \"max_depth\"\n",
    "        },\n",
    "        {\n",
    "            \"MaxValue\": \"8\",\n",
    "            \"MinValue\": \"2\",\n",
    "            \"Name\": \"min_child_weight\"\n",
    "        }\n",
    "    ]},\n",
    "    # We also need to specify how many models should be fit and how many can be fit in parallel\n",
    "    \"ResourceLimits\": {\n",
    "        \"MaxNumberOfTrainingJobs\": 21,\n",
    "        \"MaxParallelTrainingJobs\": 3\n",
    "    },\n",
    "    # Here we specify how SageMaker should update the hyperparameters as new models are fit\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    # And lastly we need to specify how we'd like to determine which models are better or worse\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "        \"MetricName\": \"validation:f1\",\n",
    "        \"Type\": \"Maximize\"\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HyperParameterTuningJobArn': 'arn:aws:sagemaker:us-east-2:815596061983:hyper-parameter-tuning-job/tuning-job2020-06-03-15-49-53',\n",
       " 'ResponseMetadata': {'RequestId': '345d93b9-44d6-4867-b27e-26dd80904430',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '345d93b9-44d6-4867-b27e-26dd80904430',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '130',\n",
       "   'date': 'Wed, 03 Jun 2020 15:49:53 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we need to choose a name for the job. This is useful for if we want to recall information about our\n",
    "# tuning job at a later date. Note that SageMaker requires a tuning job name and that the name needs to\n",
    "# be unique, which we accomplish by appending the current timestamp.\n",
    "tuning_job_name = \"tuning-job\" + strftime(\"%Y-%m-%d-%H-%M-%S\", localtime())\n",
    "\n",
    "# And now we ask SageMaker to create (and execute) the training job\n",
    "session.sagemaker_client.create_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName = tuning_job_name,\n",
    "    HyperParameterTuningJobConfig = tuning_job_config,\n",
    "    TrainingJobDefinition = training_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'HyperParameterTuningJobName': 'tuning-job2020-06-03-15-49-53',\n",
       " 'HyperParameterTuningJobArn': 'arn:aws:sagemaker:us-east-2:815596061983:hyper-parameter-tuning-job/tuning-job2020-06-03-15-49-53',\n",
       " 'HyperParameterTuningJobConfig': {'Strategy': 'Bayesian',\n",
       "  'HyperParameterTuningJobObjective': {'Type': 'Maximize',\n",
       "   'MetricName': 'validation:f1'},\n",
       "  'ResourceLimits': {'MaxNumberOfTrainingJobs': 21,\n",
       "   'MaxParallelTrainingJobs': 3},\n",
       "  'ParameterRanges': {'IntegerParameterRanges': [{'Name': 'max_depth',\n",
       "     'MinValue': '3',\n",
       "     'MaxValue': '12',\n",
       "     'ScalingType': 'Auto'},\n",
       "    {'Name': 'min_child_weight',\n",
       "     'MinValue': '2',\n",
       "     'MaxValue': '8',\n",
       "     'ScalingType': 'Auto'}],\n",
       "   'ContinuousParameterRanges': [{'Name': 'eta',\n",
       "     'MinValue': '0.05',\n",
       "     'MaxValue': '0.5',\n",
       "     'ScalingType': 'Auto'}],\n",
       "   'CategoricalParameterRanges': []}},\n",
       " 'TrainingJobDefinition': {'StaticHyperParameters': {'_tuning_objective_metric': 'validation:f1',\n",
       "   'early_stopping_rounds': '100',\n",
       "   'gamma': '4',\n",
       "   'num_round': '1000',\n",
       "   'objective': 'binary:logistic',\n",
       "   'subsample': '0.8'},\n",
       "  'AlgorithmSpecification': {'TrainingImage': '257758044811.dkr.ecr.us-east-2.amazonaws.com/sagemaker-xgboost:0.90-2-cpu-py3',\n",
       "   'TrainingInputMode': 'File',\n",
       "   'MetricDefinitions': [{'Name': 'train:mae',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-mae:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:aucpr',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-aucpr:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:merror',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-merror:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:gamma-nloglik',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-gamma-nloglik:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:mae',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-mae:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:logloss',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-logloss:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:mlogloss',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-mlogloss:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:f1',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-f1:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:accuracy',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-accuracy:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:mse',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-mse:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:poisson-nloglik',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-poisson-nloglik:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:tweedie-nloglik',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-tweedie-nloglik:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:error',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-error:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:ndcg',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-ndcg:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:map',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-map:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:auc',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-auc:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:gamma-deviance',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-gamma-deviance:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:auc',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-auc:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:error',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-error:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:merror',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-merror:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:poisson-nloglik',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-poisson-nloglik:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:rmse',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-rmse:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:logloss',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-logloss:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:accuracy',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-accuracy:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:aucpr',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-aucpr:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:tweedie-nloglik',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-tweedie-nloglik:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:rmse',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-rmse:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:gamma-deviance',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-gamma-deviance:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:mse',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-mse:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:ndcg',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-ndcg:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:f1',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-f1:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:mlogloss',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-mlogloss:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:map',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-map:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:gamma-nloglik',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-gamma-nloglik:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'ObjectiveMetric',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-f1:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'}]},\n",
       "  'RoleArn': 'arn:aws:iam::815596061983:role/service-role/AmazonSageMaker-ExecutionRole-20200514T115923',\n",
       "  'InputDataConfig': [{'ChannelName': 'train',\n",
       "    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "      'S3Uri': 's3://sagemaker-us-east-2-815596061983/kaggle/tweetSentiment/train_processed_split.csv',\n",
       "      'S3DataDistributionType': 'FullyReplicated'}},\n",
       "    'ContentType': 'csv',\n",
       "    'CompressionType': 'None'},\n",
       "   {'ChannelName': 'validation',\n",
       "    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "      'S3Uri': 's3://sagemaker-us-east-2-815596061983/kaggle/tweetSentiment/validation.csv',\n",
       "      'S3DataDistributionType': 'FullyReplicated'}},\n",
       "    'ContentType': 'csv',\n",
       "    'CompressionType': 'None'}],\n",
       "  'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-2-815596061983/kaggle/tweetSentiment/output'},\n",
       "  'ResourceConfig': {'InstanceType': 'ml.m4.xlarge',\n",
       "   'InstanceCount': 1,\n",
       "   'VolumeSizeInGB': 5},\n",
       "  'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "  'EnableNetworkIsolation': False,\n",
       "  'EnableInterContainerTrafficEncryption': False,\n",
       "  'EnableManagedSpotTraining': False},\n",
       " 'HyperParameterTuningJobStatus': 'Completed',\n",
       " 'CreationTime': datetime.datetime(2020, 6, 3, 15, 49, 53, 229000, tzinfo=tzlocal()),\n",
       " 'HyperParameterTuningEndTime': datetime.datetime(2020, 6, 3, 16, 30, 6, 91000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2020, 6, 3, 16, 30, 6, 91000, tzinfo=tzlocal()),\n",
       " 'TrainingJobStatusCounters': {'Completed': 21,\n",
       "  'InProgress': 0,\n",
       "  'RetryableError': 0,\n",
       "  'NonRetryableError': 0,\n",
       "  'Stopped': 0},\n",
       " 'ObjectiveStatusCounters': {'Succeeded': 21, 'Pending': 0, 'Failed': 0},\n",
       " 'BestTrainingJob': {'TrainingJobName': 'tuning-job2020-06-03-15-49-53-014-36db3b36',\n",
       "  'TrainingJobArn': 'arn:aws:sagemaker:us-east-2:815596061983:training-job/tuning-job2020-06-03-15-49-53-014-36db3b36',\n",
       "  'CreationTime': datetime.datetime(2020, 6, 3, 16, 11, 19, tzinfo=tzlocal()),\n",
       "  'TrainingStartTime': datetime.datetime(2020, 6, 3, 16, 13, 48, tzinfo=tzlocal()),\n",
       "  'TrainingEndTime': datetime.datetime(2020, 6, 3, 16, 16, 28, tzinfo=tzlocal()),\n",
       "  'TrainingJobStatus': 'Completed',\n",
       "  'TunedHyperParameters': {'eta': '0.3735341523449025',\n",
       "   'max_depth': '12',\n",
       "   'min_child_weight': '2'},\n",
       "  'FinalHyperParameterTuningJobObjectiveMetric': {'MetricName': 'validation:f1',\n",
       "   'Value': 0.7926120162010193},\n",
       "  'ObjectiveStatus': 'Succeeded'},\n",
       " 'ResponseMetadata': {'RequestId': '72dc158a-314b-4c3e-a570-716da23db4f4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '72dc158a-314b-4c3e-a570-716da23db4f4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '7294',\n",
       "   'date': 'Wed, 03 Jun 2020 16:30:10 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.wait_for_tuning_job(tuning_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_info = session.sagemaker_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)\n",
    "best_training_job_name = tuning_job_info['BestTrainingJob']['TrainingJobName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tuning job name.\n",
    "subprocess.check_call(\"echo {} > xgboost_model.txt\".format(best_training_job_name), shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
