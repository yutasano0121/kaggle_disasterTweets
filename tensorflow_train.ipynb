{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from time import localtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker parameters\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'kaggle/tweetSentiment'  # Prefix should not tontain '/' at the end!\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "working_dir = '/home/ec2-user/SageMaker/kaggle_data'\n",
    "data_dir = os.path.join(working_dir, 'processed_data/')\n",
    "cache_dir = os.path.join(working_dir, 'cache/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data to train and velidation data.\n",
    "train = pd.read_csv(os.path.join(data_dir, 'train_processed.csv'), header=None)\n",
    "train, validation = train_test_split(\n",
    "    train, \n",
    "    stratify=train[0].values,\n",
    "    test_size=0.25,\n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the split data to the local.\n",
    "train.to_csv(os.path.join(data_dir, 'train_processed_split.csv'), header=None, index=None)\n",
    "validation.to_csv(os.path.join(data_dir, 'validation.csv'), header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data already present in S3.\n"
     ]
    }
   ],
   "source": [
    "# data upload to S3\n",
    "s3_object_dict = s3.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=prefix\n",
    ")\n",
    "\n",
    "# Fetch filenames in the data directory.\n",
    "local_data_list = os.listdir(data_dir)\n",
    "\n",
    "# Combine the name with the S3 bucket prefix.\n",
    "local_data_list = [os.path.join(prefix, f) for f in local_data_list]\n",
    "\n",
    "# Upload the data if they are not present in S3.\n",
    "# 'data_location' is a path to a directory in which .csv files are located.\n",
    "try:\n",
    "    s3_object_list = [content['Key'] for content in s3_object_dict['Contents']]\n",
    "    if set(local_data_list).intersection(s3_object_list) == set(local_data_list):\n",
    "        train_location = os.path.join('s3://', bucket, prefix, 'train_processed_split.csv')\n",
    "        validation_location = os.path.join('s3://', bucket, prefix, 'validation.csv')\n",
    "        data_location = os.path.join('s3://', bucket, prefix)\n",
    "        \n",
    "        print(\"input_data already present in S3.\")\n",
    "        \n",
    "    else:\n",
    "        # Split the training data to train and velidation data.\n",
    "        train = pd.read_csv(os.path.join(data_dir, 'train_processed.csv'), header=None)\n",
    "        train, validation = train_test_split(\n",
    "            train, \n",
    "            stratify=train[0].values,\n",
    "            test_size=0.25,\n",
    "            random_state=0\n",
    "        )\n",
    "        \n",
    "        # Save them locally.\n",
    "        train.to_csv(os.path.join(data_dir, 'train_processed_split.csv'), header=None, index=None)\n",
    "        validation.to_csv(os.path.join(data_dir, 'validation.csv'), header=None, index=None)\n",
    "        \n",
    "        train = None,\n",
    "        validation = None\n",
    "        \n",
    "        # Upload the data to S3.\n",
    "        train_location = session.upload_data(\n",
    "            path=os.path.join(data_dir, 'train_processed_split.csv'), \n",
    "            bucket=bucket,\n",
    "            key_prefix=prefix\n",
    "        )\n",
    "        validation_location = session.upload_data(\n",
    "            path=os.path.join(data_dir, 'validation.csv'), \n",
    "            bucket=bucket,\n",
    "            key_prefix=prefix\n",
    "        )\n",
    "        data_location = os.path.join('s3://', bucket, prefix)\n",
    "\n",
    "        print(\"train and validation data uploaded to S3.\")\n",
    "        \n",
    "except KeyError:  # if nothing exists in the S3 bucket.\n",
    "    # Split the training data to train and velidation data.\n",
    "    train = pd.read_csv(os.path.join(data_dir, 'train_processed.csv'), header=None)\n",
    "    train, validation = train_test_split(\n",
    "        train, \n",
    "        stratify=train[0].values,\n",
    "        test_size=0.25,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    # Save them locally.\n",
    "    train.to_csv(os.path.join(data_dir, 'train_processed_split.csv'), header=None, index=None)\n",
    "    validation.to_csv(os.path.join(data_dir, 'validation.csv'), header=None, index=None)\n",
    "\n",
    "    train = None,\n",
    "    validation = None\n",
    "    \n",
    "    # Upload the data to S3.\n",
    "    train_location = session.upload_data(\n",
    "        path=os.path.join(data_dir, 'train_processed_split.csv'), \n",
    "        bucket=bucket,\n",
    "        key_prefix=prefix\n",
    "    )\n",
    "    validation_location = session.upload_data(\n",
    "        path=os.path.join(data_dir, 'validation.csv'), \n",
    "        bucket=bucket,\n",
    "        key_prefix=prefix\n",
    "    )\n",
    "    data_location = os.path.join('s3://', bucket, prefix)\n",
    "\n",
    "    print(\"train and validation data uploaded to S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter distribution will be renamed to {'parameter_server': {'enabled': True}} in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "# Initiate a TensorFlow instance.\n",
    "\n",
    "tf_estimator = TensorFlow(\n",
    "    entry_point='train.py',\n",
    "    source_dir='source',\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p2.xlarge',\n",
    "    framework_version='2.1.0',\n",
    "    py_version='py3',\n",
    "    distributions={'parameter_server': {'enabled': True}},\n",
    "    output_path=os.path.join('s3://', bucket, prefix, 'model'),\n",
    "    hyperparameters={\n",
    "        'input_dim': 5223,\n",
    "        'epochs': 100, \n",
    "        'batch_size': 128,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# Tune hyperparameters.\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'lr': ContinuousParameter(0.0001, 0.01),\n",
    "    'drop_rate': ContinuousParameter(0.1, 0.4),\n",
    "}  \n",
    "\n",
    "metric_definitions = [{\n",
    "    'Name': 'loss',\n",
    "    'Regex': 'loss: ([0-9\\\\.]+)'\n",
    "}]\n",
    "\n",
    "# Initialise Sagemaker's hyperparametertuner\n",
    "tuner = HyperparameterTuner(\n",
    "    tf_estimator,\n",
    "    objective_metric_name='loss',\n",
    "    objective_type='Minimize',\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=1,  # only 1 instance allowed\n",
    "    early_stopping_type='Auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "tuner.fit(\n",
    "    data_location\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow-training-200604-2204-006-ca546ec3'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.check_call('echo {} > tf_model.txt'.format(tuner.best_training_job()), shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
