{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (7613, 5)\n",
      "    train data description:\n",
      "                 id      target\n",
      "count   7613.000000  7613.00000\n",
      "mean    5441.934848     0.42966\n",
      "std     3137.116090     0.49506\n",
      "min        1.000000     0.00000\n",
      "25%     2734.000000     0.00000\n",
      "50%     5408.000000     0.00000\n",
      "75%     8146.000000     1.00000\n",
      "max    10873.000000     1.00000\n",
      "    test data shape: (3263, 4)\n",
      "    test data description:\n",
      "                 id\n",
      "count   3263.000000\n",
      "mean    5427.152927\n",
      "std     3146.427221\n",
      "min        0.000000\n",
      "25%     2683.000000\n",
      "50%     5500.000000\n",
      "75%     8176.000000\n",
      "max    10875.000000\n",
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    }
   ],
   "source": [
    "# Load data.\n",
    "train = pd.read_csv('train.csv', encoding = 'utf8')\n",
    "test = pd.read_csv('test.csv', encoding = 'utf8')\n",
    "print(\n",
    "    'train data shape: {}\\n\\\n",
    "    train data description:\\n{}\\n\\\n",
    "    test data shape: {}\\n\\\n",
    "    test data description:\\n{}'.format(\n",
    "        train.shape, train.describe(),\n",
    "        test.shape, test.describe()\n",
    "    )\n",
    ")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train.keyword.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7552"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(-pd.isnull(train.keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3342"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train.location.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5080"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(-pd.isnull(train.location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " 'One World',\n",
       " 'The barn',\n",
       " 'infj ',\n",
       " 'Texas, USA',\n",
       " 'Harlem, New York',\n",
       " 'Portoviejo-Manabi-Ecuador',\n",
       " 'Long Island NY & San Francisco',\n",
       " 'Trinidad and Tobago',\n",
       " 'New Jersey ',\n",
       " 'PG County, MD',\n",
       " 'Ikeja, Nigeria',\n",
       " 'PA.USA',\n",
       " 'Hampshire UK',\n",
       " ' New England',\n",
       " 'on twitter ',\n",
       " 'Calgary, AB, Canada',\n",
       " 'toronto, ontario',\n",
       " ' Queensland, Australia',\n",
       " 'Yuba City, CA',\n",
       " 'North East USA',\n",
       " 'Kawartha Lakes, Ontario, Canad',\n",
       " 'Warsaw',\n",
       " 'Port Charlotte, FL',\n",
       " 'UK',\n",
       " 'My old New England home',\n",
       " 'MA via PA',\n",
       " 'Indonesia',\n",
       " 'San Diego, California',\n",
       " 'California, USA',\n",
       " 'Petaluma, CA',\n",
       " 'ÌÏT: 19.123127,72.825133',\n",
       " 'Pacific Northwest',\n",
       " 'Wellington',\n",
       " 'brisbane, australia',\n",
       " 'Scotland, United Kingdom',\n",
       " 'Eureka, California, USA',\n",
       " 'Kaneohe',\n",
       " 'Dreieich, Germany',\n",
       " 'North Dartmouth, Massachusetts',\n",
       " 'Calgary, Alberta, Canada',\n",
       " 'In Your Notifications ',\n",
       " 'Born in Baltimore Living in PA',\n",
       " 'somewhere over a rainbow',\n",
       " 'University of Chicago',\n",
       " 'Upstate New York',\n",
       " 'Menlo Park. SFO. The World.',\n",
       " 'Quezon City, Philippines',\n",
       " 'calgary,ab',\n",
       " 'Hawaii USA',\n",
       " 'Your notifications',\n",
       " 'My subconscious',\n",
       " 'Whippany, NJ',\n",
       " 'Spain - China - Latin America.',\n",
       " 'Ab, Canada',\n",
       " 'tokyo',\n",
       " 'LONG ISLAND, NY',\n",
       " 'WORLD',\n",
       " 'Jammu and Kashmir',\n",
       " 'on twitter',\n",
       " 'The Memesphere',\n",
       " 'NJ/NYC',\n",
       " 'Absecon, NJ',\n",
       " 'LA ??',\n",
       " 'right next to you',\n",
       " 'Inside the Beltway (DC Area)',\n",
       " 'US, PA',\n",
       " 'In the spirit world',\n",
       " 'big boy \\x89Û¢ 0802',\n",
       " 'Meereen ',\n",
       " 'Savannah, GA',\n",
       " 'NYC metro',\n",
       " 'Jerusalem',\n",
       " 'Central Coast, California',\n",
       " 'In Space',\n",
       " 'Warrandyte, Australia',\n",
       " 'Florida',\n",
       " 'Chicago, IL 60607',\n",
       " '11202',\n",
       " 'Bhubneshwar',\n",
       " 'a feminist, modernist hag.',\n",
       " 'UK Great Britain ',\n",
       " 'Somewhere Out There',\n",
       " 'Birmingham, United Kingdom',\n",
       " 'The land of New Jersey. ',\n",
       " 'Virginia, United States',\n",
       " 'Queens, NY',\n",
       " 'London, Greater London, UK',\n",
       " 'The shores of Lake Kilby',\n",
       " 'Aperture Science Test Facility',\n",
       " 'Duval, WV 25573, USA ?',\n",
       " 'Epic City, BB.',\n",
       " 'Pompano Beach, FL',\n",
       " 'New York Brooklyn',\n",
       " 'Ottawa, Canada',\n",
       " 'Indiana',\n",
       " 'Pawnee',\n",
       " 'JDB/LJC/AGB/TW/PLL',\n",
       " 'CamaquÌ£/Pelotas',\n",
       " 'The D',\n",
       " 'I Heard #2MBikers',\n",
       " 'Thrissur',\n",
       " 'Suplex City',\n",
       " \"'Merica\",\n",
       " 'Newcastle, England',\n",
       " 'The Windy City',\n",
       " 'Paris, France',\n",
       " 'New York, NY',\n",
       " 'Playa',\n",
       " 'Morganville, Texas.',\n",
       " 'Zeerust, South Africa',\n",
       " 'Austin, TX',\n",
       " 'Houston, TX',\n",
       " '518',\n",
       " 'Fort Valley,GA/Fayetteville,AR',\n",
       " 'Gurgaon, Haryana. ',\n",
       " 'New Mexico, USA',\n",
       " 'PSN: Pipbois ',\n",
       " 'Neverland',\n",
       " 'Fresno, CA',\n",
       " 'Nigeria, Global',\n",
       " 'Auckland, New Zealand',\n",
       " 'St. Louis, Mo',\n",
       " 'dallas',\n",
       " 'Some Where in this World',\n",
       " 'Palma, Islas Baleares',\n",
       " '\\x89Û¢OlderCandyBloom\\x89Û¢',\n",
       " 'Brisbane Australia',\n",
       " 'Chasing My Dreams w/Jass??',\n",
       " 'sydney, australia',\n",
       " 'Greenville,SC',\n",
       " 'Athens - Nicosia',\n",
       " 'Bangalore, INDIA',\n",
       " 'Campinas Sp',\n",
       " 'St Austell, Cornwall',\n",
       " 'WORLDWIDE!',\n",
       " 'Perth, Western Australia',\n",
       " 'Inverness, Nova Scotia',\n",
       " 'Rafael castillo',\n",
       " 'Im In Route ',\n",
       " 'SoCal',\n",
       " 'Worcester, MA',\n",
       " 'Bloomington, Indiana',\n",
       " 'africa',\n",
       " 'Everett, WA',\n",
       " 'Littleton, CO, USA',\n",
       " 'Pueblo, CO',\n",
       " 'CA',\n",
       " '17-Feb',\n",
       " 'Wilmington, NC',\n",
       " 'Earth',\n",
       " '876 Jamrock.',\n",
       " 'Houston TX',\n",
       " 'he/him or she/her (ask)',\n",
       " 'Port Williams NS',\n",
       " 'Chevy Chase, MD',\n",
       " 'Cydia',\n",
       " 'MÌ©xico D.F.',\n",
       " 'Orlando, Fl',\n",
       " 'Gotham City',\n",
       " 'Hensley Street, Portland',\n",
       " 'Sydney, New South Wales',\n",
       " 'KLA,Uganda',\n",
       " 'Walthamstow, London',\n",
       " 'Telangana',\n",
       " 'Chicago, Il',\n",
       " 'Rapid City, Black Hills, SD',\n",
       " 'Peterborough, Ont.',\n",
       " 'Boston, Massachusetts',\n",
       " 'MUM-DEL',\n",
       " 'ona block w/ my BOY ??',\n",
       " 'Canberra, Australian Capital Territory',\n",
       " 'Roaming around the world',\n",
       " 'Henderson, Nevada',\n",
       " 'The Desert',\n",
       " 'Franklin, TN near Nashville',\n",
       " 'The Jewfnited State',\n",
       " '#PhanTrash',\n",
       " 'Nottingham, England',\n",
       " '48.870833,2.399227',\n",
       " 'seattle wa',\n",
       " 'In Hell',\n",
       " 'Dutch/English/German',\n",
       " 'OK',\n",
       " 'Washington, DC 20009',\n",
       " 'Asia European Continent Korea ',\n",
       " 'los angeles, ca',\n",
       " 'Alvin, TX',\n",
       " 'Brazos Valley, Texas',\n",
       " 'Belfast',\n",
       " 'sneaking glances at Thancred',\n",
       " 'livin in a plastic world',\n",
       " 'Nunya',\n",
       " 'Lucknow, India',\n",
       " 'Kingston, Jamaica',\n",
       " 'ÌÏT: 43.631838,-79.55807',\n",
       " 'a van down by the river',\n",
       " 'EastCarolina',\n",
       " 'New York - Connecticut',\n",
       " 'Hermosa Beach, CA',\n",
       " 'atlanta',\n",
       " 'Noida, NCR, India',\n",
       " 'Geneva. And beyond. ',\n",
       " 'QUEENS.',\n",
       " '5-Feb',\n",
       " 'South Stand',\n",
       " 'Spring Tx',\n",
       " 'wny',\n",
       " '??t?a',\n",
       " 'Dubai',\n",
       " 'Your Conversation',\n",
       " 'sweden',\n",
       " 'Here there and everywhere',\n",
       " '#MadeInNorthumberland',\n",
       " 'South Carolina',\n",
       " 'Wisconsin, USA',\n",
       " 'timeline kamu',\n",
       " 'Brasil, Fortaleza ce',\n",
       " 'philly ',\n",
       " 'ARIZONA',\n",
       " 'The Circle of Life',\n",
       " 'Fairfax, VA',\n",
       " 'England ',\n",
       " 'Chappaqua NY and Redlands CA',\n",
       " 'moss chamber b',\n",
       " 'Lansdale,Pennsylvania',\n",
       " 'uk',\n",
       " 'with Doflamingo',\n",
       " 'Hickville, USA',\n",
       " 'Mumbai, Maharashtra',\n",
       " 'right here',\n",
       " 'Bishops Stortford, England',\n",
       " 'Hogsmeade',\n",
       " 'Stalybridge, Tameside',\n",
       " 'taken by piper curda',\n",
       " 'San Luis Obispo, CA',\n",
       " 'taking pain like pleasure',\n",
       " 'Corpus - Las Vegas - Houston',\n",
       " 'Austin',\n",
       " 'Pratt-on-Wye',\n",
       " 'Aurora, Ontario ',\n",
       " 'houston',\n",
       " 'London, UK',\n",
       " 'Jakarta',\n",
       " 'Ktx',\n",
       " 'rzl ?',\n",
       " 'Where the money at',\n",
       " 'Also follow ?',\n",
       " 'Manchester, UK',\n",
       " 'South africa',\n",
       " \"in Dimitri's arms\",\n",
       " 'Derbyshire, United Kingdom',\n",
       " 'ÌÏT: 1.50225,103.742992',\n",
       " 'Amman,Jordan',\n",
       " 'Madisonville TN',\n",
       " '\\x89Û¢III.XII.MMXI\\x89Û¢',\n",
       " ' Alex/Mika/Leo|18|he/she/they',\n",
       " 'Kettering, OH',\n",
       " 'Dudetown',\n",
       " 'potters bar',\n",
       " 'Wolmers Trust School for Boys ',\n",
       " 'fl',\n",
       " 'lakewood colorado',\n",
       " 'new york, ny',\n",
       " 'Anderson, SC',\n",
       " 'West Bank, Gaza Strip',\n",
       " 'Asia Pacific   ',\n",
       " '????????????',\n",
       " 'Melbourne, Australia',\n",
       " 'Nowhere Islands/Smash Manor',\n",
       " 'Mumbai',\n",
       " 'Providence RI / Lisnaskea ',\n",
       " 'Northern California U.S.A.',\n",
       " 'Aveiro, Portugal',\n",
       " 'Valle Del Sol',\n",
       " 'Loading...',\n",
       " 'Saint Paul',\n",
       " 'Chiswick, London',\n",
       " 'Mass',\n",
       " 'Traverse City, MI',\n",
       " 'Johannesburg, South Africa ',\n",
       " 'FIMAK A.S Ist Bolge Muduru',\n",
       " 'Palestine ',\n",
       " 'Depok',\n",
       " 'Tampa-St. Petersburg, FL',\n",
       " 'El Dorado, Arkansas',\n",
       " 'Ottawa, Ontario',\n",
       " 'In the Shadows...',\n",
       " 'nigeria',\n",
       " 'Regalo Island',\n",
       " 'Delhi ',\n",
       " 'Lakewood, Tennessee',\n",
       " 'cognitive dissonance town',\n",
       " 'Birmingham',\n",
       " 'Calgary',\n",
       " 'Phoenix, AZ',\n",
       " 'Ely, Cambridgeshire',\n",
       " 'en el pais de los arrechos',\n",
       " 'Victoria, British Columbia',\n",
       " 'Orange County, California',\n",
       " 'NYC :) Ex- #Islamophobe',\n",
       " 'Making Worldwide Change Near U',\n",
       " '??',\n",
       " 'http://www.amazon.com/dp/B00HR',\n",
       " 'El Dorado, KS',\n",
       " 'West Wales',\n",
       " 'Bahrain',\n",
       " 'Colorado Springs',\n",
       " 'Chile',\n",
       " 'Bristol',\n",
       " 'iamdigitalent.com',\n",
       " 'Lynnfield, MA',\n",
       " 'Greenwich Meridian',\n",
       " 'Evanston, IL',\n",
       " 'golborne, north west england.',\n",
       " 'in my own personal hell (:',\n",
       " 'dorito land',\n",
       " 'y/e/l',\n",
       " '140920-21 & 150718-19 BEIJING',\n",
       " 'santo domingo',\n",
       " 'Ellensburg to Spokane',\n",
       " 'ÌÏT: 33.209923,-87.545328',\n",
       " 'Afghanistan',\n",
       " 'Based out of Portland, Oregon',\n",
       " 'Orlando',\n",
       " 'nor*cal',\n",
       " 'southwest, Tx',\n",
       " 'From NY. In Scranton, PA',\n",
       " 'Stowmarket',\n",
       " 'North Carolina ',\n",
       " 'Cypress, CA 90630',\n",
       " 'TÌÁchira - Venezuela',\n",
       " 'Tokyo & Osaka',\n",
       " 'Sligo and Galway, Ireland',\n",
       " 'San Antonio, TX',\n",
       " 'BOT ACCOUNT',\n",
       " '709?',\n",
       " 'Nagpur',\n",
       " 'Lagos Nigeria',\n",
       " 'Antioch, CA ',\n",
       " 'Mount Vernon, NY',\n",
       " 'Nashville',\n",
       " 'Rochester',\n",
       " 'Jonesboro, AR MO, IOWA USA',\n",
       " 'ÌÏT: 10.614817868480726,12.195582811791382',\n",
       " 'Miami Beach, Fl',\n",
       " 'Antarctica',\n",
       " ' The World',\n",
       " 'yorkshire\\n',\n",
       " '3???2???????',\n",
       " 'The TARDIS',\n",
       " 'San Francisco , CA',\n",
       " 'Bathtub de Bett ',\n",
       " 'Displaced Son of TEXAS!',\n",
       " 'Bandar Lampung, Indonesia',\n",
       " 'Photo : Blue Mountains ',\n",
       " 'London/Lagos/FL ÌÏT: 6.6200132,',\n",
       " 'NAIROBI  KENYA ',\n",
       " '18 \\x89Û¢ CC',\n",
       " 'Silang, Cavite / ParaÌ±aque',\n",
       " '#otrakansascity',\n",
       " 'Lynwood, CA',\n",
       " 'The Wood',\n",
       " 'LIVERPOOL',\n",
       " 'Hartford,  connecticut',\n",
       " 'L. A.',\n",
       " 'Bangalore',\n",
       " 'watford',\n",
       " 'statesboro/vidalia',\n",
       " 'Ontario Canada',\n",
       " 'Paris (France)',\n",
       " 'Berlin, Germany',\n",
       " 'Christiana,Tennessee',\n",
       " 'swindon',\n",
       " 'Location',\n",
       " 'Santiago de Cmpostela Galicia',\n",
       " 'Sand springs oklahoma',\n",
       " 'Dimapur',\n",
       " 'Wild Wild Web',\n",
       " 'Groton, CT',\n",
       " 'West Chester, PA',\n",
       " 'Bay Area',\n",
       " 'Sunny South florida ',\n",
       " 'Wanderlust',\n",
       " 'Galveston, Texas',\n",
       " 'Kleenex factory',\n",
       " 'Silicon Valley',\n",
       " 'Olathe, KS',\n",
       " '19.600858, -99.047821',\n",
       " 'West Midlands',\n",
       " 'San Diego CA',\n",
       " 'VCU',\n",
       " 'Bridport, England',\n",
       " '9/1/13',\n",
       " 'Oslo, Norway',\n",
       " 'EIC',\n",
       " 'everydaynigerian@gmail.com',\n",
       " 'San Diego, Calif.',\n",
       " 'W.I.T.S Academy',\n",
       " ' Jariana Town',\n",
       " 'West Lancashire, UK.',\n",
       " 'LITTLETON, CO, USA, TERRAN',\n",
       " 'Fort Myers, Florida',\n",
       " 'Yobe State',\n",
       " 'Erbil',\n",
       " 'Copenhagen, Capital Region of Denmark',\n",
       " 'Somewhere ',\n",
       " 'Unknown ',\n",
       " 'Swindon,England ',\n",
       " 'PDX',\n",
       " 'LA - everywhere',\n",
       " 'Justin and Ariana follow',\n",
       " 'CPT & JHB, South Africa',\n",
       " 'ATLANTA , GEORGIA ',\n",
       " 'Swag Francisco',\n",
       " 'Headed To The Top',\n",
       " 'Asheboro, NC',\n",
       " 'Trinity, Bailiwick of Jersey',\n",
       " 'US',\n",
       " '#1 Vacation Destination,HAWAII',\n",
       " 'NC',\n",
       " 'Benedict College',\n",
       " 'Instagram: trillrebel_',\n",
       " 'Heathrow',\n",
       " 'Minneapolis, MN',\n",
       " 'North Hastings Ontario',\n",
       " '???????? ?????????.',\n",
       " '21.462446,-158.022017',\n",
       " 'Saline, MI',\n",
       " 'Buffalo NY',\n",
       " 'ÌÏT: 40.562796,-75.488849',\n",
       " 'Melbourne-ish',\n",
       " 'Idaho',\n",
       " 'Jerusalem!',\n",
       " 'West Hollywood',\n",
       " 'WestEnd, Puritan Ave ',\n",
       " 'Fairfield, California',\n",
       " 'mexico',\n",
       " 'EspaÌ±a - Spain - Espagne',\n",
       " 'Clayton, NC',\n",
       " 'Federal Capital Territory',\n",
       " 'Surulere Lagos,Home Of Swagg',\n",
       " \"In @4SkinChan 's arms\",\n",
       " 'Lyallpur, Pakistan',\n",
       " 'Michigan',\n",
       " 'Sale, England',\n",
       " 'Cochrane, Alberta, Canada',\n",
       " 'Lisbon, Portugal',\n",
       " 'Kashmir!',\n",
       " 'UK,singer,songwriter,?2 act',\n",
       " 'Macclesfield',\n",
       " 'Adelaide, South Australia',\n",
       " 'california mermaid ? ',\n",
       " 'San Diego, CA',\n",
       " 'The Citadel, Oldtown, Westeros',\n",
       " \"FSC '19\",\n",
       " 'Toledo, OH',\n",
       " 'Top secret bunker ',\n",
       " 'Watch Those Videos -',\n",
       " 'Dalston, Hackney',\n",
       " '#goingdownthetoilet Illinois',\n",
       " 'Pune, mostly ',\n",
       " 'Henderson, NV',\n",
       " 'Hagerstown, MD',\n",
       " 'Lima, Peru',\n",
       " 'Kenton, Ohio',\n",
       " 'Reddit ',\n",
       " 'NBO',\n",
       " 'The Hammock, FL, USA',\n",
       " '\\x89Û¢901\\x89Û¢',\n",
       " 'Washington, DC & Charlotte, NC',\n",
       " 'Numenor',\n",
       " 'Minneapolis/St. Paul',\n",
       " '#EngleWood CHICAGO ',\n",
       " 'todaysbigstock.com',\n",
       " 'Nevada (wishing for Colorado)',\n",
       " 'Haddonfield, NJ',\n",
       " 'North Highlands, CA',\n",
       " 'Vancouver, BC, Canada',\n",
       " 'Las Vegas, Nevada',\n",
       " 'London.',\n",
       " \"Viterbo BFA Acting '18\",\n",
       " '?? ?+254? ? \\\\??å¡_??å¡_???å¡_?/??',\n",
       " 'WV, love the blue and gold',\n",
       " 'Saskatchewan, Canada',\n",
       " 'Where ever i please',\n",
       " 'Georgia',\n",
       " 'melbourne',\n",
       " 'Marbella. Spain',\n",
       " 'Dakar',\n",
       " 'A Hoop Somewhere',\n",
       " 'Charleston, WV',\n",
       " 'Jackson',\n",
       " 'USA/SO FLORIDA via BROOKLYN NY',\n",
       " 'Los Angeles, London, Kent',\n",
       " 'Washington state',\n",
       " 'victoria mozÌ£o ',\n",
       " '772 Temperance Permenence',\n",
       " 'Dalkeith, Scotland',\n",
       " 'Las Vegas aka Hell',\n",
       " 'Arundel ',\n",
       " 'front row at a show',\n",
       " 'china',\n",
       " 'LFC x GSW',\n",
       " 'Pontevedra, Galicia',\n",
       " 'Alberta, VA',\n",
       " 'Jamaica',\n",
       " 'between ideas & 3-5pm AEST',\n",
       " '21, Porto',\n",
       " 'The American Wasteland (MV)',\n",
       " 'Madison, WI & St. Louis MO',\n",
       " 'Detroit/Windsor',\n",
       " \"@protectingtitan's side.\",\n",
       " 'Carol Stream, Illinois',\n",
       " 'Nashville, Tn',\n",
       " 'hertfordshire.',\n",
       " 'Everywhere',\n",
       " '#ODU',\n",
       " 'The Internetz',\n",
       " 'Mumbai, India',\n",
       " 'Derby',\n",
       " 'Kalimantan Timur, Indonesia',\n",
       " 'Leeds',\n",
       " ' Alberta',\n",
       " 'At Da Laundry Mat Wit Nivea ',\n",
       " 'Sydney Australia',\n",
       " 'I-75 in Florida',\n",
       " 'Hoxton, London',\n",
       " 'Tractor land aka Bristol',\n",
       " 'Montreal',\n",
       " 'Puerto Rico',\n",
       " 'Tafekop Ga-Matsepe',\n",
       " 'Jubail IC, Saudi Arabia',\n",
       " 'Football Field',\n",
       " 'Bloomington, IN',\n",
       " 'ANYWEHERE !!',\n",
       " 'w. Nykae ',\n",
       " 'Georgia, U.S.A.',\n",
       " 'San Diego',\n",
       " \"satan's colon\",\n",
       " 'Le Memenet',\n",
       " 'Calgary, Canada',\n",
       " 'Dublin, Ireland',\n",
       " 'Karachi ',\n",
       " 'Israel',\n",
       " 'Rockford, IL',\n",
       " 'CAMARILLO, CA',\n",
       " 'Kenosha, WI 53143',\n",
       " '?????? ??? ?????? ????????',\n",
       " 'Arizona',\n",
       " 'Wahpeton, ND',\n",
       " 'Queen Creek AZ',\n",
       " 'Boston, MA',\n",
       " 'Decatur, GA',\n",
       " '#Gladiator \\x89Û¢860\\x89Û¢757\\x89Û¢',\n",
       " 'Charlotte, N.C.',\n",
       " 'Norf Carolina',\n",
       " 'Screwston, TX',\n",
       " 'New York, NY ',\n",
       " 'Peterborough, On',\n",
       " 'on the go',\n",
       " 'In my studio',\n",
       " 'Dallas Fort-Worth',\n",
       " 'austin tx',\n",
       " 'the local dump',\n",
       " 'VÌ_sterÌ´s, Sweden',\n",
       " 'Salt Lake City, UT',\n",
       " 'TN',\n",
       " 'Numa casa de old yellow bricks',\n",
       " 'Suburban Detroit, Michigan',\n",
       " 'Extraterrestrial Highway',\n",
       " 'Malaysia',\n",
       " 'Gold Coast',\n",
       " 'ÌÏT: 40.707762,-74.014213',\n",
       " 'Williamstown, VT',\n",
       " 'Moscow',\n",
       " 'NYC, New York',\n",
       " 'Minority Privilege, USA',\n",
       " 'in my head',\n",
       " 'too far',\n",
       " 'åÊ(?\\x89Û¢`?\\x89Û¢å«)??',\n",
       " 'SF Bay Area, California / Greater Phoenix, AZ',\n",
       " 'mnl',\n",
       " 'Wilmington, DE',\n",
       " 'Iowa, USA',\n",
       " 'WAISTDEEP, TX',\n",
       " 'Halfrica',\n",
       " 'We are global!',\n",
       " 'All around the world',\n",
       " 'Honduras',\n",
       " '?Gangsta OC / MV RP; 18+.?',\n",
       " 'portland, oregon',\n",
       " 'Layang-Layang, Perak',\n",
       " 'Lancaster, Pennsylvania, USA',\n",
       " 'Alberta ',\n",
       " 'sisterhood',\n",
       " 'At your back',\n",
       " 'Anchorage, AK',\n",
       " 'Indianapolis, IN',\n",
       " 'Manchester, England',\n",
       " 'Winston Salem, North Carolina',\n",
       " 'Central Florida',\n",
       " 'Pioneer Village, KY',\n",
       " 'buffalo / madrid / granada',\n",
       " 'Nashville, TN',\n",
       " 'Sacramento, CA',\n",
       " 'Warri',\n",
       " 'Newcastle upon Tyne',\n",
       " 'Paranaque City',\n",
       " 'Tucson, Az',\n",
       " 'Georgia, USA',\n",
       " 'Charleston S.C.',\n",
       " 'Fort Walton Beach, Fl',\n",
       " 'Hartford, Connecticut',\n",
       " 'Michigan, USA',\n",
       " 'In the Shadows',\n",
       " 'Trinidad & Tobago',\n",
       " 'Clean World',\n",
       " 'Atlanta',\n",
       " 'music.',\n",
       " 'Bartholomew County, Indiana',\n",
       " 'Gotham City,USA',\n",
       " 'Italy',\n",
       " '253',\n",
       " 'Stockholm, Sweden',\n",
       " 'Banbridge',\n",
       " 'Hailing from Dayton ',\n",
       " '???  Dreamz',\n",
       " 'Waco TX',\n",
       " 'West Coast, Cali USA',\n",
       " 'Freeport IL. USA',\n",
       " 'Wales',\n",
       " 'Garrett',\n",
       " 'Ljubljana, Slovenia',\n",
       " 'Oshawa, Canada',\n",
       " 'Tacoma,Washington',\n",
       " 'Boston MA',\n",
       " 'From a torn up town MANCHESTER',\n",
       " 'London, Kent & SE England.',\n",
       " '[Gia.] | #KardashianEmpire',\n",
       " 'Louisiana, USA',\n",
       " 'Virginia',\n",
       " 'ill yorker',\n",
       " 'North Cack/919',\n",
       " 'Minneapolis - St. Paul',\n",
       " 'USA , AZ',\n",
       " 'Walker County, Alabama',\n",
       " 'Dappar (Mohali) Punjab',\n",
       " 'Macon, GA',\n",
       " 'rome',\n",
       " 'Hope Road, Jamaica ',\n",
       " 'not so cool KY',\n",
       " 'trapped in America',\n",
       " 'Chicago,Illinois',\n",
       " 'Toronto, Worldwide ',\n",
       " 'h+l',\n",
       " 'Financial News and Views',\n",
       " 'Atlantic Highlands, NJ',\n",
       " 'iPhone: -27.499212,153.011072',\n",
       " 'Vancouver, British Columbia',\n",
       " '#KaumElite;#F?VOR;#SMOFC',\n",
       " 'Michigan ',\n",
       " 'somewhere too cold for me',\n",
       " 'Kabul, Tuebingen, Innsbruck',\n",
       " 'South, USA',\n",
       " 'Ashland, Oregon',\n",
       " 'Dayton, Ohio',\n",
       " 'Buffalo, NY',\n",
       " 'Lancashire, United Kingdom',\n",
       " 'Cassadaga Florida',\n",
       " 'St. Catharines, Ontario',\n",
       " 'Jupiter',\n",
       " 'Linton Hall, VA',\n",
       " 'Bulgaria',\n",
       " 'Rhyme Or Reason?',\n",
       " 'GLOBAL/WORLDWIDE',\n",
       " 'PanamÌÁ ',\n",
       " 'Massachusetts ',\n",
       " 'Did anybody see me here ??',\n",
       " 'Islamabad',\n",
       " 'Jerseyville, IL',\n",
       " \"Yuuko-san's shop\",\n",
       " 'The South & WestCoast ',\n",
       " 'Horsemind, MI',\n",
       " 'dmv ?? fashion school @ KSU. ',\n",
       " 'Dhaka, Bangladsh',\n",
       " 'Gwersyllt, Wales',\n",
       " 'Dhaka',\n",
       " 'Canterbury kent',\n",
       " 'Riverview, FL ',\n",
       " 'Brisbane.',\n",
       " 'Fort Worth,  Texas ',\n",
       " 'West Palm Beach, Florida',\n",
       " \"???????, ??'??????\",\n",
       " 'SE London(heart is by the sea)',\n",
       " 'Pedophile hunting ground',\n",
       " 'In My Lab Creating ',\n",
       " 'Tarragona',\n",
       " 'Miami?Gainesville',\n",
       " 'El Paso, TX',\n",
       " 'Live On Webcam',\n",
       " 'Alexandria, Egypt.',\n",
       " 'Ocean City, NJ',\n",
       " 'Shirley, NY',\n",
       " 'Some pum pum',\n",
       " 'Rochelle, GA',\n",
       " 'Piedmont Triad, NC',\n",
       " 'Dundee',\n",
       " 'England & Wales Border, UK',\n",
       " 'Ottawa,Ontario Canada',\n",
       " 'Minneapolis,MN,US',\n",
       " 'Kingston, Pennsylvania',\n",
       " 'Hannover, Germany',\n",
       " 'Coventry',\n",
       " 'Top Secret',\n",
       " 'Birmingham, England',\n",
       " 'Hattiesburg, MS',\n",
       " 'West Hollywood, CA',\n",
       " 'Torry Alvarez love forever ? ?',\n",
       " 'Carterville',\n",
       " 'japon',\n",
       " 'Sugar Land, TX',\n",
       " 'Selangor',\n",
       " 'milky way',\n",
       " 'Australian Capital Territory',\n",
       " 'Nebraska, Colorado & The GLOBE',\n",
       " 'Chicago',\n",
       " 'Hemel Hempstead',\n",
       " 'they/her',\n",
       " 'wherever-the-fuck washington',\n",
       " 'Columbia Heights, MN',\n",
       " 'ÌÏT: 6.4682,3.18287',\n",
       " '#expelcl*y',\n",
       " 'Lehigh Valley, PA',\n",
       " 'Tampa, FL',\n",
       " 'MontrÌ©al, QuÌ©bec',\n",
       " '#keepthefaith J&J',\n",
       " 'Fort Wayne, IN',\n",
       " 'T-Ville',\n",
       " 'heart of darkness, unholy ?',\n",
       " 'Los Angeles ',\n",
       " 'Heaven',\n",
       " 'Oldenburg // London',\n",
       " 'kissimmee,fl.',\n",
       " 'Hertfordshire ',\n",
       " 'lagos. Unilag',\n",
       " '65',\n",
       " 'Based in CA - Serve Nationwide',\n",
       " 'Freddy Fazbears pizzeria',\n",
       " 'Oklahoma, USA',\n",
       " 'Sioux Falls, SD',\n",
       " 'Kernow',\n",
       " 'Baker City Oregon',\n",
       " '92',\n",
       " \"Dime's Palace\",\n",
       " 'Reddit',\n",
       " 'Southern California',\n",
       " 'South, England',\n",
       " 'nap central',\n",
       " 'lost in history',\n",
       " '?? Cloud Mafia ??',\n",
       " 'prob turning up with sheen',\n",
       " 'Vista, CA',\n",
       " 'Essex, England',\n",
       " 'dundalk ireland',\n",
       " 'Medford, Oregon',\n",
       " 'Kuala Lumpur, Malaysia',\n",
       " 'Pune, Maharashtra',\n",
       " 'bk. ',\n",
       " 'Orm',\n",
       " 'Lima-Peru',\n",
       " 'The P (South Philly)',\n",
       " 'Wakanda',\n",
       " 'Team Slytherin',\n",
       " 'Aracaju - Sergipe',\n",
       " '$ad $hawty',\n",
       " 'Jersey',\n",
       " 'Eau Claire, Wisconsin',\n",
       " 'New Jersey',\n",
       " 'Rochester Hills, MI',\n",
       " 'St Charles, MD',\n",
       " 'Youngstown, OH',\n",
       " 'Colorado, USA',\n",
       " 'india',\n",
       " 'Geneva',\n",
       " 'New York, USA',\n",
       " 'Voorhees, NJ',\n",
       " 'Rock Springs, WY',\n",
       " 'Berlin - Germany',\n",
       " 'Niagara Falls, Ontario',\n",
       " 'plymouth',\n",
       " 'St Paul, MN',\n",
       " \"'SAN ANTONIOOOOO'\",\n",
       " 'BrasÌ_lia',\n",
       " 'My mind is my world',\n",
       " ' New Delhi ',\n",
       " 'ARBAILO',\n",
       " 'Njoro, Kenya',\n",
       " 'Philippines',\n",
       " 'heccfidmss@gmail.com',\n",
       " 'Baltimore',\n",
       " 'Arizona ',\n",
       " 'Melrose',\n",
       " 'USA, Haiti, Nepal',\n",
       " 'God.Family.Money',\n",
       " 'EARTH ',\n",
       " 'Bagalkote Karnataka ',\n",
       " 'CT, USA',\n",
       " 'D(M)V  ',\n",
       " 'manchester, uk.',\n",
       " 'Atlanta, Georgia',\n",
       " 'Madison, Wisconsin, USA',\n",
       " 'Somewhere Powerbraking A Chevy',\n",
       " 'Port Jervis, NY',\n",
       " 'Atlantic, IA',\n",
       " 'Proudly Canadian!',\n",
       " 'GO BLUE! HAIL YES!!',\n",
       " 'oxford',\n",
       " 'Miami, Florida',\n",
       " 'gaffney, sc ',\n",
       " 'MY RTs ARE NOT ENDORSEMENTS',\n",
       " 'online ',\n",
       " 'ITALY',\n",
       " ' Eugene, Oregon',\n",
       " 'Desde Republica Argentina',\n",
       " '#WashingtonState #Seattle',\n",
       " 'Perthshire ',\n",
       " 'Finland',\n",
       " 'Lubbock, Texas',\n",
       " 'Hollywood',\n",
       " 'Live mÌÁs',\n",
       " 'Hughes, AR',\n",
       " 'Newport, Wales, UK',\n",
       " 'Dundee, UK',\n",
       " 'Fashion Heaven. IG: TMId_',\n",
       " 'New Jersey, usually',\n",
       " 'ÌÏT: 36.142163,-95.979189',\n",
       " 'AKRON OHIO USA',\n",
       " 'West Vancouver, B.C.',\n",
       " 'Reading UK',\n",
       " 'Montana ',\n",
       " 'Bishops Lydeard, England',\n",
       " 'israel',\n",
       " 'ON',\n",
       " 'Rochester, NY',\n",
       " 'Varies ',\n",
       " 'A.A.S my Aztec Princess',\n",
       " 'Patra-Greece.',\n",
       " '[@blackparavde is my frankie]',\n",
       " 'Charlotte ',\n",
       " 'WORLDWIDE-BOSTON',\n",
       " 'å_: ?? ÌÑ ? : ?',\n",
       " 'Arvada, CO',\n",
       " 'Charlotte, North Carolina',\n",
       " 'pissing off antis',\n",
       " 'sÌ£o luis',\n",
       " 'NYC / International',\n",
       " 'Phoenix Az',\n",
       " 'DC',\n",
       " ' snapchat // fvck_casper ',\n",
       " 'In a crazy genius mind',\n",
       " 'New Delhi, India',\n",
       " 'Maryland, USA',\n",
       " 'Illinois',\n",
       " 'St Joseph de Beauce',\n",
       " 'Laventillemoorings ',\n",
       " 'toledo',\n",
       " 'texas a&m university',\n",
       " 'English Midlands',\n",
       " 'Insula Barataria',\n",
       " 'Kodiak, AK',\n",
       " 'SF Bay Area',\n",
       " 'Stratford, CT',\n",
       " 'Manila',\n",
       " 'Bow, NH',\n",
       " 'california | oregon | peru |',\n",
       " 'North Ferriby, East Yorkshire',\n",
       " 'Hinterestland',\n",
       " 'The Waystone Inn',\n",
       " 'Very SW CA, USA....Draenor',\n",
       " 'Nairobi',\n",
       " 'VitÌ_ria (ES)',\n",
       " \"Jakarta/Kuala Lumpur/S'pore\",\n",
       " '3rd Eye Chakra',\n",
       " 'Athens,Greece',\n",
       " 'Tucson, AZ',\n",
       " 'somewhere in Portugal',\n",
       " 'Wynne, AR',\n",
       " '?semekeepschanging@soyeh?',\n",
       " 'EveryWhere',\n",
       " 'Kualar Lumpur, Malaysia',\n",
       " 'Timaru District, New Zealand',\n",
       " 'California',\n",
       " 'San Francisco Bay Area',\n",
       " 'In #Fairie, where else? ;-)',\n",
       " 'IJmuiden, The Netherlands',\n",
       " 'where the wild things are',\n",
       " 'Coolidge, AZ',\n",
       " 'Jamshedpur, Jharkhand',\n",
       " \"St. Patrick's Purgatory\",\n",
       " \"'soooota\",\n",
       " 'Cape Town, Khayelitsha',\n",
       " 'KADUNA, NIGERIA',\n",
       " 'Alberta Pack',\n",
       " 'North-East Region, Singapore',\n",
       " '#iminchina',\n",
       " \"DRAW A CIRCLE THAT'S THE EARTH\",\n",
       " 'North Carolina',\n",
       " 'Wrigley Field',\n",
       " 'Quincy',\n",
       " 'Thane',\n",
       " 'Sicamous, British Columbia',\n",
       " 'Va Beach, Virginia',\n",
       " 'Valparaiso ',\n",
       " 'Skyport de la Rosa',\n",
       " 'Around the world',\n",
       " 'Boston/Montreal ',\n",
       " 'Augusta, Maine, 04330',\n",
       " 'Right here',\n",
       " 'Nice places ',\n",
       " 'Knoxville, TN',\n",
       " 'Western Washington',\n",
       " 'Lahore',\n",
       " 'cork',\n",
       " 'å¡å¡Midwest \\x89Û¢\\x89Û¢',\n",
       " 'Riverside, California.',\n",
       " 'y(our) boyfriends legs ',\n",
       " 'Johannesburg, South Africa',\n",
       " 'Somerset, UK',\n",
       " 'please H? ?:??',\n",
       " 'Elkhart, IN',\n",
       " 'Yogya Berhati Nyaman',\n",
       " 'Milwaukee WI',\n",
       " 'Bakersfield, CA',\n",
       " 'Huntsville, AL',\n",
       " 'Ireland',\n",
       " \"Dil's Campsite\",\n",
       " 'Danbury, CT',\n",
       " '3?3?7?SLOPelousas??2?2?5?',\n",
       " 'Huntington, WV',\n",
       " 'somewhere USA ',\n",
       " 'Hawaii, USA',\n",
       " \"401 livin'\",\n",
       " 'The Pig Sty',\n",
       " '?205?478?',\n",
       " 'Fort Walton Beach, FL',\n",
       " 'lugo',\n",
       " 'Riyadh',\n",
       " 'San Diego California 92101',\n",
       " \"Someday I'll live in England. \",\n",
       " 'The Main ',\n",
       " 'Buxton, Venice, and Nottingham',\n",
       " 'Aberdeenshire',\n",
       " 'Sunbury, Ohio',\n",
       " 'Definitely NOT the stables',\n",
       " 'Illumination ',\n",
       " 'Dallas, TX',\n",
       " 'Garden Grove',\n",
       " 'instagram- Chloe_Bellx',\n",
       " 'Rutherfordton, NC',\n",
       " 'Waterfront',\n",
       " 'Tipperary (Long Way) ',\n",
       " \"United States where it's warm\",\n",
       " 'Eaubonne, 95, France',\n",
       " '#HAMont',\n",
       " 'Korea',\n",
       " '302',\n",
       " 'Cornwall',\n",
       " 'Pittsburgh',\n",
       " 'California or Colorado',\n",
       " 'Richmond Heights, OH',\n",
       " 'Greenpoint, Brooklyn',\n",
       " 'Alphen aan den Rijn, Holland',\n",
       " 'ca(NADA) ',\n",
       " 'SWinfo@dot.state.al.us',\n",
       " 'everywhere ',\n",
       " \"R'lyeh, South Pacific\",\n",
       " 'Kansas City, Mo.',\n",
       " 'the Refrigerator ',\n",
       " 'Cascadia',\n",
       " 'Hatteras, North Carolina',\n",
       " 'Cloud 9',\n",
       " '?????? ???? ??????',\n",
       " 'Renfrew, Scotland',\n",
       " 'San Juan, Puerto Rico',\n",
       " 'hyderabad',\n",
       " 'ATL ? SEA ',\n",
       " 'nj/ny',\n",
       " 'Tallahassee Florida',\n",
       " 'Chennai',\n",
       " 'Cape Town',\n",
       " 'midwest',\n",
       " 'jeddah | Khartoum',\n",
       " 'guaravitas',\n",
       " 'Okuma Town, Fukushima',\n",
       " 'Scottsdale. AZ',\n",
       " 'cedar rapids ia',\n",
       " 'IDN',\n",
       " 'paradise',\n",
       " 'Wisconsin',\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train.location.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "222 unique `keyword` values (including `nan`) out of 7522 non-nan values.<br>\n",
    "One-hot encode it.<br><br>\n",
    "On the other hand, 3342 unique `location` values out of 5080 non-nan values.<br>\n",
    "Seems correct location names and meaningless location names are mixed, but not sure how to separate them.<br>\n",
    "For now, just make it a column of one-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'nan' with 'none'\n",
    "def replaceNAN_none(df, column='keyword'):\n",
    "    col_edit = df[column].values.copy()\n",
    "    col_edit[pd.isnull(col_edit)] = 'none'\n",
    "    col_edit = col_edit.reshape(-1, 1)\n",
    "    return col_edit\n",
    "\n",
    "train_keyword_edit = replaceNAN_none(train)\n",
    "test_keyword_edit = replaceNAN_none(test)\n",
    "\n",
    "# Set up an one-hot encoder.\n",
    "enc = OneHotEncoder(handle_unknown='ignore', dtype=np.int)  # Ignore categories not present in the training data.\n",
    "enc.fit(train_keyword_edit)\n",
    "\n",
    "def replaceCol_onehot(df, column='keyword', encoder=enc):\n",
    "    col_edit = replaceNAN_none(df, column)\n",
    "    cols_onehot = encoder.transform(col_edit).toarray()\n",
    "    \n",
    "    df_new = df.drop(column, axis=1)\n",
    "    df_new = pd.concat([df_new, pd.DataFrame(cols_onehot)], axis=1)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "    \n",
    "train = replaceCol_onehot(train)\n",
    "test = replaceCol_onehot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train.columns.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'location' column and drop the original with 'id' column.\n",
    "train['location_isnull'] = pd.isnull(train.location).astype('int')\n",
    "test['location_isnull'] = pd.isnull(test.location).astype('int')\n",
    "train.drop(['id', 'location'], axis=1, inplace=True)\n",
    "test.drop(['id', 'location'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Our Deeds are the Reason of this #earthquake M...\n",
       "1                  Forest fire near La Ronge Sask. Canada\n",
       "2       All residents asked to 'shelter in place' are ...\n",
       "3       13,000 people receive #wildfires evacuation or...\n",
       "4       Just got sent this photo from Ruby #Alaska as ...\n",
       "5       #RockyFire Update => California Hwy. 20 closed...\n",
       "6       #flood #disaster Heavy rain causes flash flood...\n",
       "7       I'm on top of the hill and I can see a fire in...\n",
       "8       There's an emergency evacuation happening now ...\n",
       "9       I'm afraid that the tornado is coming to our a...\n",
       "10            Three people died from the heat wave so far\n",
       "11      Haha South Tampa is getting flooded hah- WAIT ...\n",
       "12      #raining #flooding #Florida #TampaBay #Tampa 1...\n",
       "13                #Flood in Bago Myanmar #We arrived Bago\n",
       "14      Damage to school bus on 80 in multi car crash ...\n",
       "15                                         What's up man?\n",
       "16                                          I love fruits\n",
       "17                                       Summer is lovely\n",
       "18                                      My car is so fast\n",
       "19                           What a goooooooaaaaaal!!!!!!\n",
       "20                                 this is ridiculous....\n",
       "21                                      London is cool ;)\n",
       "22                                            Love skiing\n",
       "23                                  What a wonderful day!\n",
       "24                                               LOOOOOOL\n",
       "25                         No way...I can't eat that shit\n",
       "26                                  Was in NYC last week!\n",
       "27                                     Love my girlfriend\n",
       "28                                              Cooool :)\n",
       "29                                     Do you like pasta?\n",
       "                              ...                        \n",
       "7583    Pic of 16yr old PKK suicide bomber who detonat...\n",
       "7584    These boxes are ready to explode! Exploding Ki...\n",
       "7585    Calgary Police Flood Road Closures in Calgary....\n",
       "7586    #Sismo DETECTADO #JapÌ_n 15:41:07 Seismic inte...\n",
       "7587                                   Sirens everywhere!\n",
       "7588    BREAKING: #ISIS claims responsibility for mosq...\n",
       "7589                                       Omg earthquake\n",
       "7590    SEVERE WEATHER BULLETIN No. 5 FOR: TYPHOON ÛÏ...\n",
       "7591    Heat wave warning aa? Ayyo dei. Just when I pl...\n",
       "7592    An IS group suicide bomber detonated an explos...\n",
       "7593    I just heard a really loud bang and everyone i...\n",
       "7594    A gas thing just exploded and I heard screams ...\n",
       "7595    NWS: Flash Flood Warning Continued for Shelby ...\n",
       "7596    RT @LivingSafely: #NWS issues Severe #Thunders...\n",
       "7597    #??? #?? #??? #??? MH370: Aircraft debris foun...\n",
       "7598    Father-of-three Lost Control of Car After Over...\n",
       "7599    1.3 #Earthquake in 9Km Ssw Of Anza California ...\n",
       "7600    Evacuation order lifted for town of Roosevelt:...\n",
       "7601    #breaking #LA Refugio oil spill may have been ...\n",
       "7602    a siren just went off and it wasn't the Forney...\n",
       "7603    Officials say a quarantine is in place at an A...\n",
       "7604    #WorldNews Fallen powerlines on G:link tram: U...\n",
       "7605    on the flip side I'm at Walmart and there is a...\n",
       "7606    Suicide bomber kills 15 in Saudi security site...\n",
       "7607    #stormchase Violent Record Breaking EF-5 El Re...\n",
       "7608    Two giant cranes holding a bridge collapse int...\n",
       "7609    @aria_ahrary @TheTawniest The out of control w...\n",
       "7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
       "7611    Police investigating after an e-bike collided ...\n",
       "7612    The Latest: More Homes Razed by Northern Calif...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make bag of words from tweet texts\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def tweet_to_words(text):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text() # Remove HTML tags.\n",
    "    text = re.sub(r\"[,\\.]\", \"\", text)  # Remove ',' and '.'.\n",
    "    text = re.sub(r\"(\\w?)\\d+(\\w?)\", \"\\\\1 thisissomenumber \\\\2\", text)  # Replace all number words with a token text.\n",
    "    text = re.sub(r\"\\d+\", \"thisissomenumber\", text)  # Replace all number words with a token text.\n",
    "    text = re.sub(r\"[^a-zA-Z0-9']\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    words = ' '.join(words)  # Make it back into a sentence\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ss thisissomenumber '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='ss12'\n",
    "re.sub(r\"(\\w?)\\d+(\\w?)\", \"\\\\1 thisissomenumber \\\\2\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deed reason earthquak may allah forgiv us'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_to_words(train.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_train = train.text.apply(tweet_to_words)\n",
    "words_test = test.text.apply(tweet_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            deed reason earthquak may allah forgiv us\n",
       "1                 forest fire near la rong sask canada\n",
       "2    resid ask 'shelter place' notifi offic evacu s...\n",
       "3    thisissomenumb thisissomenumb peopl receiv wil...\n",
       "4    got sent photo rubi alaska smoke wildfir pour ...\n",
       "5    rockyfir updat california hwi thisissomenumb t...\n",
       "6    flood disast heavi rain caus flash flood stree...\n",
       "7                           i'm top hill see fire wood\n",
       "8        there' emerg evacu happen build across street\n",
       "9                         i'm afraid tornado come area\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "cache_dir = 'cache/'\n",
    "cache_file = 'bagOfWords.pkl'\n",
    "\n",
    "def extract_BoW_features(\n",
    "    words_train, words_test, \n",
    "    vocabulary_size=5000,\n",
    "    cache_dir=cache_dir, cache_file=cache_file\n",
    "):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        print(\"Make bag of words from scratch.\")\n",
    "        # Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features=vocabulary_size)\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        print(\"Load bag of words from cache.\")\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read features from cache file: bagOfWords.pkl\n",
      "Load bag of words from cache.\n"
     ]
    }
   ],
   "source": [
    "bow_train, bow_test, vocabulary = extract_BoW_features(words_train, words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target  0  1  2  3  4  5  6  7  8  ...  4990  4991  4992  4993  4994  4995  \\\n",
      "0       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
      "1       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
      "2       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
      "3       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
      "4       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
      "\n",
      "   4996  4997  4998  4999  \n",
      "0     0     0     0     0  \n",
      "1     0     0     0     0  \n",
      "2     0     0     0     0  \n",
      "3     0     0     0     0  \n",
      "4     0     0     0     0  \n",
      "\n",
      "[5 rows x 5224 columns]\n",
      "   0     1     2     3     4     5     6     7     8     9     ...  4990  \\\n",
      "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "3     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "4     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "\n",
      "   4991  4992  4993  4994  4995  4996  4997  4998  4999  \n",
      "0     0     0     0     0     0     0     0     0     0  \n",
      "1     0     0     0     0     0     0     0     0     0  \n",
      "2     0     0     0     0     0     0     0     0     0  \n",
      "3     0     0     0     0     0     0     0     0     0  \n",
      "4     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 5223 columns]\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([train, pd.DataFrame(bow_train)], axis=1)\n",
    "test = pd.concat([test, pd.DataFrame(bow_test)], axis=1)\n",
    "train.drop('text', axis=1, inplace=True)\n",
    "test.drop('text', axis=1, inplace=True)\n",
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>4990</th>\n",
       "      <th>4991</th>\n",
       "      <th>4992</th>\n",
       "      <th>4993</th>\n",
       "      <th>4994</th>\n",
       "      <th>4995</th>\n",
       "      <th>4996</th>\n",
       "      <th>4997</th>\n",
       "      <th>4998</th>\n",
       "      <th>4999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5224 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  0  1  2  3  4  5  6  7  8  ...  4990  4991  4992  4993  4994  4995  \\\n",
       "0       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "1       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "2       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "3       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "4       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "\n",
       "   4996  4997  4998  4999  \n",
       "0     0     0     0     0  \n",
       "1     0     0     0     0  \n",
       "2     0     0     0     0  \n",
       "3     0     0     0     0  \n",
       "4     0     0     0     0  \n",
       "\n",
       "[5 rows x 5224 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the target values the first column.\n",
    "train_y = train['target']\n",
    "train_x = train.drop('target', axis=1)\n",
    "train = pd.concat([train_y, train_x], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"processed_data/train_processed.csv\", header=False, index=False)\n",
    "test.to_csv(\"processed_data/test_processed.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing\n",
      "    train shape: (7613, 5224)\n",
      "    test shape: (3263, 5223)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"After preprocessing\\n\\\n",
    "    train shape: {}\\n\\\n",
    "    test shape: {}\".format(\n",
    "        train.shape,\n",
    "        test.shape\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
