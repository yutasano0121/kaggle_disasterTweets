{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (7613, 5)\n",
      "    train data description:\n",
      "                 id      target\n",
      "count   7613.000000  7613.00000\n",
      "mean    5441.934848     0.42966\n",
      "std     3137.116090     0.49506\n",
      "min        1.000000     0.00000\n",
      "25%     2734.000000     0.00000\n",
      "50%     5408.000000     0.00000\n",
      "75%     8146.000000     1.00000\n",
      "max    10873.000000     1.00000\n",
      "    test data shape: (3263, 4)\n",
      "    test data description:\n",
      "                 id\n",
      "count   3263.000000\n",
      "mean    5427.152927\n",
      "std     3146.427221\n",
      "min        0.000000\n",
      "25%     2683.000000\n",
      "50%     5500.000000\n",
      "75%     8176.000000\n",
      "max    10875.000000\n",
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    }
   ],
   "source": [
    "# Load data.\n",
    "train = pd.read_csv('train.csv', encoding = 'utf8')\n",
    "test = pd.read_csv('test.csv', encoding = 'utf8')\n",
    "print(\n",
    "    'train data shape: {}\\n\\\n",
    "    train data description:\\n{}\\n\\\n",
    "    test data shape: {}\\n\\\n",
    "    test data description:\\n{}'.format(\n",
    "        train.shape, train.describe(),\n",
    "        test.shape, test.describe()\n",
    "    )\n",
    ")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train.keyword.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7552"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(-pd.isnull(train.keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3342"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train.location.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5080"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(-pd.isnull(train.location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " 'North Carolina',\n",
       " 'Republic of the Philippines',\n",
       " 'Frostburg',\n",
       " 'EGYPT',\n",
       " 'RhodeIsland',\n",
       " 'Peterborough, Ontario, Canada',\n",
       " 'Kuwait',\n",
       " 'Atlantic Highlands, NJ',\n",
       " 'Chicagoland',\n",
       " 'Nevada, USA',\n",
       " 'Ohio, USA',\n",
       " 'Malibu/SantaFe/Winning!',\n",
       " 'East Kilbride',\n",
       " 'Screwston, TX',\n",
       " 'Newark, NJ',\n",
       " 'Jerusalem',\n",
       " '? ',\n",
       " 'Colorado, USA',\n",
       " 'Den Helder, Rijkswerf',\n",
       " 'Free State, South Africa',\n",
       " 'lagos nigeria',\n",
       " 'Montana ',\n",
       " 'Rome, Italy',\n",
       " 'Huntsville, Alabama',\n",
       " 'San Luis Obispo, CA',\n",
       " 'Estados Unidos',\n",
       " 'Toronto',\n",
       " 'yorkshire\\n',\n",
       " 'ARGENTINA',\n",
       " 'Konoha',\n",
       " 'Michigan, USA',\n",
       " 'Rocky Mountains',\n",
       " 'El Dorado, Arkansas',\n",
       " 'Ondo',\n",
       " 'GLOBAL/WORLDWIDE',\n",
       " 'Largo, MD',\n",
       " 'China',\n",
       " 'Orm',\n",
       " 'on to the next adventure',\n",
       " 'rzl ?',\n",
       " 'W.I.T.S Academy',\n",
       " 'Torrance, CA',\n",
       " 'Kalimantan Timur, Indonesia',\n",
       " 'Broomfield, CO',\n",
       " 'Caracas, Venezuela.',\n",
       " 'Reading UK',\n",
       " 'See the barn of bleakness',\n",
       " 'Ely, Cambridgeshire',\n",
       " 'Oklahoma, USA',\n",
       " 'New Hampshire',\n",
       " 'Birmingham, UK',\n",
       " '#keepthefaith J&J',\n",
       " 'Buscame EL tu Melte',\n",
       " 'Tacoma,Washington',\n",
       " 'Bombardment Bay',\n",
       " 'Huber Heights, OH',\n",
       " 'New York, United States',\n",
       " '73101',\n",
       " 'Storybrooke / The Moors',\n",
       " 'IndiLand ',\n",
       " 'NYC,US - Cali, Colombia',\n",
       " 'UAE,Sharjah/ AbuDhabi',\n",
       " 'NIGERIA',\n",
       " 'Arlington, VA and DC',\n",
       " 'QUEENS.',\n",
       " 'Stockholm, Sweden',\n",
       " 'Tampa',\n",
       " 'Mexico City',\n",
       " 'EARTH ',\n",
       " 'Pig Symbol, Alabama',\n",
       " 'Gidi',\n",
       " 'brooklyn, NYC',\n",
       " 'Jersey City, New Jersey',\n",
       " '\\x89Û¢ Views From The Six \\x89Û¢',\n",
       " 'Nunya',\n",
       " 'District of Gentrification/ DC',\n",
       " 'Oxford / bristol',\n",
       " 'Pawnee',\n",
       " 'Fort Collins, CO',\n",
       " 'Canada ',\n",
       " 'Bolton & Tewkesbury, UK',\n",
       " 'Behind The Obama Curtain',\n",
       " 'dmv ?? fashion school @ KSU. ',\n",
       " 'The Triskelion',\n",
       " 'Youngstown, OH',\n",
       " 'Amsterdam & Worldwide',\n",
       " 'Horsemind, MI',\n",
       " 'At Da Laundry Mat Wit Nivea ',\n",
       " 'Mostly Wellington, NZ ',\n",
       " 'Miami ??',\n",
       " 'UK Great Britain ',\n",
       " '627',\n",
       " 'Jogja, Indonesia Slowly Asia',\n",
       " 'Spring Tx',\n",
       " 'N. California USA',\n",
       " 'Carol Stream, Illinois',\n",
       " 'ÌÏT: 35.223347,-80.827834',\n",
       " 'columbus ohio',\n",
       " 'Kwajalein/Virginia/Dayton, OH',\n",
       " 'Birmingham, United Kingdom',\n",
       " 'Chicago Heights, IL',\n",
       " 'ona block w/ my BOY ??',\n",
       " 'Unite. Bless. Wallahi ',\n",
       " 'Georgia, U.S.A.',\n",
       " 'Dubai',\n",
       " '?semekeepschanging@soyeh?',\n",
       " 'BestCoast',\n",
       " 'Saline, MI',\n",
       " 'Louisiana',\n",
       " 'Ventura, Ca',\n",
       " 'CA via Brum',\n",
       " 'AUSTRALIA-SOUTHAFRICA-CAMBODIA',\n",
       " 'Cedar Island, Clinton CT 06413',\n",
       " 'Amsterdam | San Francisco',\n",
       " '[@blackparavde is my frankie]',\n",
       " 'CPT & JHB, South Africa',\n",
       " 'Purfleet',\n",
       " 'ARIZONA',\n",
       " '#Gladiator \\x89Û¢860\\x89Û¢757\\x89Û¢',\n",
       " 'Concord, CA',\n",
       " 'Frascati',\n",
       " 'Nigeria ',\n",
       " 'instagram- Chloe_Bellx',\n",
       " 'Global Edition',\n",
       " 'Amman, Jordan',\n",
       " 'Pembroke NH',\n",
       " 'Phoenix',\n",
       " 'Bayonne, NJ',\n",
       " 'Dallas, TX',\n",
       " 'Central Illinois',\n",
       " 'Port Charlotte, FL',\n",
       " 'West Hollywood',\n",
       " 'San Juan, Puerto Rico',\n",
       " 'Valle Del Sol',\n",
       " 'Visit our  dedicated website @',\n",
       " 'Columbus, Georgia',\n",
       " 'Las Cruces, NM',\n",
       " 'Manhattan, NY',\n",
       " 'labuan, malaysia',\n",
       " 'SÌ£o Paulo, Brasil',\n",
       " 'Laredo, TX',\n",
       " 'Cambridge, Massachusetts, U.S.',\n",
       " 'Miami, FL',\n",
       " 'Manila City',\n",
       " 'West Wales',\n",
       " 'Eldoret, kenya',\n",
       " 'East London. ',\n",
       " 'Beaumont, TX',\n",
       " 'Lake Monticello, VA',\n",
       " 'Pretoria',\n",
       " 'STL ?NOLA',\n",
       " 'Chicago Area',\n",
       " 'ph',\n",
       " 'Plano, Texas',\n",
       " 'New Mexico, USA',\n",
       " 'Odawara, Japan',\n",
       " 'Alphen aan den Rijn, Holland',\n",
       " 'Hudson Valley, NY',\n",
       " 'Holly Springs, NC ',\n",
       " 'Santiago Bernabeau',\n",
       " 'Delhi ',\n",
       " 'Tucson, Arizona ',\n",
       " 'Kauai, Hawaii',\n",
       " 'The green and pleasant land.',\n",
       " 'Queen Creek AZ',\n",
       " 'Concord, NH ',\n",
       " 'Your notifications',\n",
       " 'the insane asylum. ',\n",
       " \"satan's colon\",\n",
       " 'london essex england uk',\n",
       " 'Bay Area',\n",
       " 'Marietta, GA',\n",
       " 'hatena bookmark',\n",
       " 'The shores of Lake Kilby',\n",
       " 'Sydney, New South Wales',\n",
       " 'Netherlands',\n",
       " 'Cuttack, Orissa',\n",
       " '?????? ???? ??????',\n",
       " 'Lancaster California',\n",
       " 'Houston, TX',\n",
       " \"DRAW A CIRCLE THAT'S THE EARTH\",\n",
       " '65',\n",
       " 'Stay Tuned ;) ',\n",
       " 'scumbernauld',\n",
       " 'Greenfield, Massachusetts',\n",
       " 'belleville',\n",
       " 'Corpus Christi, Texas',\n",
       " 'Wales',\n",
       " 'Oklahoma',\n",
       " 'North London',\n",
       " 'London / Berlin / Online',\n",
       " 'Amarillo',\n",
       " 'Mogadishu, Somalia',\n",
       " 'London/Lagos/FL ÌÏT: 6.6200132,',\n",
       " 'Displaced Son of TEXAS!',\n",
       " 'In Your Notifications ',\n",
       " 'Melbourne, FL',\n",
       " 'Former Yugoslav Republic of Macedonia',\n",
       " 'london town..',\n",
       " 'Saint Paul',\n",
       " 'The Jewfnited State',\n",
       " 'israel',\n",
       " 'Queensland',\n",
       " 'Athens - Nicosia',\n",
       " 'Jeddah_Saudi Arabia.',\n",
       " 'Brackley Beach, PE, Canada',\n",
       " 'SaudI arabia - riyadh ',\n",
       " 'Breaking News',\n",
       " 'lakewood colorado',\n",
       " 'Alexandria, VA, USA',\n",
       " 'Dundas, Ontario',\n",
       " 'Watertown, Mass.',\n",
       " 'sindria',\n",
       " 'Earth ',\n",
       " 'Norway',\n",
       " 'sri lanka',\n",
       " 'lowestoft',\n",
       " 'Ashford, Kent, United Kingdom',\n",
       " 'Gold Coast, Qld, Australia',\n",
       " 'a box',\n",
       " 'Hollywood, CA',\n",
       " 'St Louis, MO',\n",
       " 'Unknown ',\n",
       " 'Hollywood',\n",
       " 'Highland Park, CA',\n",
       " 'Walthamstow, London',\n",
       " 'NIFC',\n",
       " 'New Orleans, LA',\n",
       " 'Bristol, UK',\n",
       " 'Rochester, NY',\n",
       " 'Wausau, Wisconsin',\n",
       " 'Arlington, TX',\n",
       " 'Aveiro, Portugal',\n",
       " 'Massachusetts',\n",
       " 'Hickville, USA',\n",
       " 'International',\n",
       " 'Tornado Alley, USA ',\n",
       " 'home ',\n",
       " 'Nicoma Park, OK',\n",
       " 'Magnolia, Fiore ',\n",
       " 'TÌÁchira - Venezuela',\n",
       " 'UK  & Germany',\n",
       " 'Wolmers Trust School for Boys ',\n",
       " 'Orlando,FL  USA',\n",
       " 'Haiku, Maui, Hawaii',\n",
       " 'leyland',\n",
       " 'The Multiverse',\n",
       " 'The Forever Girl',\n",
       " 'Aarhus, Central Jutland',\n",
       " 'Tucson, AZ',\n",
       " 'Harpurhey, Manchester, UK',\n",
       " 'Bartholomew County, Indiana',\n",
       " 'Kuala Lumpur',\n",
       " '???????, Texas',\n",
       " 'probably petting an animal',\n",
       " 'Medford, NJ',\n",
       " 'Street of Dallas',\n",
       " 'Based in CA - Serve Nationwide',\n",
       " 'England,UK,Europe,Sol 3.',\n",
       " 'SURROUNDED BY WEEABOOS',\n",
       " 'Colonial Heights, VA',\n",
       " 'Philippines',\n",
       " 'DMV',\n",
       " 'Houston TX',\n",
       " 'everywhere',\n",
       " 'Leeds, England',\n",
       " 'Santo Domingo Alma Rosa ',\n",
       " 'Rocketing through the galaxy',\n",
       " ' Miami Beach',\n",
       " 'Rochester',\n",
       " 'Gameday',\n",
       " 'NY',\n",
       " 'Magnolia',\n",
       " 'Durand, MI',\n",
       " 'SÌ£o Paulo',\n",
       " 'World news',\n",
       " 'Greensboro, North Carolina',\n",
       " 'Aztec NM',\n",
       " 'lee london',\n",
       " 'Dundee',\n",
       " 'Harlingen, TX',\n",
       " 'Helsinki, Finland',\n",
       " 'Tn',\n",
       " 'she/her/your majesty/empress',\n",
       " 'Palo Alto, California',\n",
       " \"Viterbo BFA Acting '18\",\n",
       " 'Kwara, Nigeria',\n",
       " 'Njoro, Kenya',\n",
       " 'OK',\n",
       " 'Freeport il ',\n",
       " 'maryland',\n",
       " ' Jariana Town',\n",
       " 'Portsmouth, VA',\n",
       " 'Waukesha, WI',\n",
       " 'PH',\n",
       " 'Davidson, NC',\n",
       " 'Reality',\n",
       " 'between ideas & 3-5pm AEST',\n",
       " 'Brighton and Hove',\n",
       " 'AKRON OHIO USA',\n",
       " '@notoriousD12',\n",
       " 'Ab, Canada',\n",
       " \"St. Patrick's Purgatory\",\n",
       " 'not so cool KY',\n",
       " 'Temecula, CA',\n",
       " 'Honolulu,Hawaii ',\n",
       " 'Bracknell',\n",
       " 'im definitely taller than you.',\n",
       " 'Evansville, IN',\n",
       " 'Fort Walton Beach, Fl',\n",
       " 'Vancouver, BC, Canada',\n",
       " 'Cornwall',\n",
       " 'North East / Middlesbrough ',\n",
       " 'Rockford, IL',\n",
       " 'Geneva',\n",
       " 'West Bank, Gaza Strip',\n",
       " 'Wanderlust',\n",
       " 'Hermitage, PA',\n",
       " 'Hamburg, DE',\n",
       " 'EspÌ_rito Santo',\n",
       " 'nashville, tn ',\n",
       " 'New Britain, CT',\n",
       " 'Beautiful British Columbia',\n",
       " 'CORNFIELDS',\n",
       " 'Fukushima city Fukushima.pref',\n",
       " 'Long Eaton åá Derbyshire åá UK',\n",
       " 'At your back',\n",
       " '1648 Queen St. West, Toronto.',\n",
       " 'dreamy lake',\n",
       " 'Telangana',\n",
       " 'hollywoodland ',\n",
       " 'Greenpoint, Brooklyn',\n",
       " 'Itirapina, SÌ£o Paulo',\n",
       " 'come here in 20 minutes for an ass kicking',\n",
       " 'Lubbock, TX',\n",
       " 'Arthas US',\n",
       " 'Home of the Takers.',\n",
       " 'BILASPUR,CHHATTISGARH,495001',\n",
       " 'Santiago,RepÌ¼blica Dominicana',\n",
       " 'Japan',\n",
       " '#HarleyChick#PJNT#RunBenRun',\n",
       " 'canada',\n",
       " 'http://www.amazon.com/dp/B00HR',\n",
       " 'Somewhere else...',\n",
       " 'Keighley, England',\n",
       " 'Wynne, AR',\n",
       " 'Bokaro Steel City, Jharkhand',\n",
       " 'Birmingham, England',\n",
       " \"Jakarta/Kuala Lumpur/S'pore\",\n",
       " 'Kenton, Ohio',\n",
       " 'Hammersmith, London',\n",
       " 'My mind is my world',\n",
       " 'MÌ©rida, YucatÌÁn',\n",
       " 'Cochrane, Alberta, Canada',\n",
       " 'Arvada, CO',\n",
       " 'Alberta, VA',\n",
       " 'Arnhem, the Netherlands',\n",
       " 'the own zone layer ',\n",
       " \"'Merica\",\n",
       " 'Hensley Street, Portland',\n",
       " '? icon by @Hashiren_3 ?',\n",
       " 'On a beach ',\n",
       " 'Eagle Mountain, Texas ',\n",
       " '\\x89Û¢FLG\\x89Û¢',\n",
       " 'ATL ??',\n",
       " 'Overland Park, KS',\n",
       " 'Mostly Yuin.',\n",
       " '46.950109,7.439469',\n",
       " 'GLOBAL',\n",
       " 'Mongolia',\n",
       " 'YA MOTHA BED',\n",
       " 'Somewhere between here & there',\n",
       " 'Fakefams',\n",
       " 'Team Slytherin',\n",
       " 'Miami?Gainesville',\n",
       " '3?3?7?SLOPelousas??2?2?5?',\n",
       " 'Azeroth',\n",
       " 'In #Fairie, where else? ;-)',\n",
       " 'Southern California',\n",
       " 'port matilda pa',\n",
       " 'BrasÌ_lia',\n",
       " 'Flipadelphia',\n",
       " 'Roanoke VA',\n",
       " 'Western Washington',\n",
       " '1/10 Taron squad',\n",
       " '570 Vanderbilt; Brooklyn, NY',\n",
       " 'Okanagan Valley, BC',\n",
       " 'Lakewood, Tennessee',\n",
       " 'the Dirty D',\n",
       " 'PURPLE BOOTH STUDIO\\x89ã¢',\n",
       " 'Kodiak, AK',\n",
       " 'Born in Baltimore Living in PA',\n",
       " 'South Asia',\n",
       " 'Hinterestland',\n",
       " 'Venezuela',\n",
       " '??? ??? ????? ??? ???.',\n",
       " 'JKT48-Muse-A7X',\n",
       " 'NY, CT & Greece',\n",
       " 'Varies ',\n",
       " \"Wherever I'm sent\",\n",
       " '{Detailed}',\n",
       " 'infj ',\n",
       " 'Philadelphia',\n",
       " 'CLVLND',\n",
       " 'SEA Server',\n",
       " 'Cape Town, Khayelitsha',\n",
       " 'Henderson, Nevada',\n",
       " '#UNITE THE BLUE  ',\n",
       " 'Ocean City, NJ',\n",
       " 'antoine fisher ',\n",
       " 'he/him or she/her (ask)',\n",
       " \"I'm standing behind you\",\n",
       " 'America of Founding Fathers',\n",
       " 'New Orleans, Louisiana',\n",
       " 'BiÌ±an,Laguna',\n",
       " 'Am International',\n",
       " 'Global',\n",
       " 'Yellowknife, NT',\n",
       " 'Manila',\n",
       " 'Playa',\n",
       " 'Sunnyvale, CA',\n",
       " 'Amman,Jordan',\n",
       " 'Roppongi, Minato, Tokyo ',\n",
       " 'buhh',\n",
       " 'ill yorker',\n",
       " 'CA physically- Boston Strong?',\n",
       " 'Bend, Oregon',\n",
       " 'Galveston, Texas',\n",
       " 'milky way',\n",
       " 'dope show',\n",
       " 'World Wide Web',\n",
       " 'Uppsala, Sweden',\n",
       " 'WV, love the blue and gold',\n",
       " 'Lincoln',\n",
       " 'Memphis,TN/ World Wide',\n",
       " 'New Chicago',\n",
       " 'This Is Paradise. Relax. ',\n",
       " 'Tipperary (Long Way) ',\n",
       " 'Pro-American and Anti-#Occupy',\n",
       " 'Financial News and Views',\n",
       " 'Louisville, KY ',\n",
       " 'Port Harcourt, Nigeria',\n",
       " 'Portland, Ore. ',\n",
       " 'Sheffield Township, Ohio',\n",
       " 'New York, USA',\n",
       " 'Erbil',\n",
       " 'rio de janeiro | brazil',\n",
       " 'South Africa',\n",
       " 'Sacramento, CA',\n",
       " 'Tyler, TX',\n",
       " '17-Feb',\n",
       " 'New York 2099',\n",
       " 'All around the world baby',\n",
       " 'Austin | San Diego',\n",
       " '#MadeInNorthumberland',\n",
       " 'mainly California',\n",
       " 'Midwest',\n",
       " 'Lynchburg, VA',\n",
       " 'Charlotte',\n",
       " '?? ?+254? ? \\\\??å¡_??å¡_???å¡_?/??',\n",
       " 'Up a hill',\n",
       " '(a) property of the universe',\n",
       " 'New Your',\n",
       " 'In my own world!!!',\n",
       " 'Palermo, Sicily',\n",
       " 'Alabama, USA',\n",
       " 'Some other mansion',\n",
       " 'Pomfret/Providence',\n",
       " 'LEALMAN, FLORIDA',\n",
       " 'PARACHUTE',\n",
       " 'Sao Paulo, Brazil',\n",
       " 'Email: Lovethterry@gmail.com',\n",
       " 'Sandton, South Africa',\n",
       " 'La Puente, CA',\n",
       " 'Pelham, AL',\n",
       " 'Depok',\n",
       " 'Killafornia made me ',\n",
       " 'Jubail IC, Saudi Arabia',\n",
       " 'Nottingham, England',\n",
       " 'st.louis county missouri ',\n",
       " 'Tampa-St. Petersburg, FL',\n",
       " 'Lansdale,Pennsylvania',\n",
       " 'SWMO',\n",
       " 'Melbourne Australia',\n",
       " 'Hartford,  connecticut',\n",
       " 'ITALY',\n",
       " 'La Grange Park, IL',\n",
       " 'seattle grace mercy death',\n",
       " 'Pleasanton, CA',\n",
       " 'ECSU16',\n",
       " '-?s?s?j??s-',\n",
       " 'Wiltshire',\n",
       " 'Ames, Iowa',\n",
       " 'everywhere ',\n",
       " \"Me mammy's belly\",\n",
       " 'Winston Salem, North Carolina',\n",
       " 'Porto Alegre, Rio Grande do Sul',\n",
       " 'Utica NY',\n",
       " 'Earth: Senseless nonsense',\n",
       " 'London/Bristol/Guildford',\n",
       " 'cody, austin follows ?*?',\n",
       " 'Overton NV',\n",
       " 'Suginami-ku, Tokyo, Japan',\n",
       " 'Waddesdon',\n",
       " 'RSN: Tru',\n",
       " 'Hagerstown, MD',\n",
       " 'Calgary, Alberta, Canada',\n",
       " 'IJmuiden, The Netherlands',\n",
       " 'Some where',\n",
       " 'Northampton, MA',\n",
       " 'North Carolina, USA',\n",
       " 'Toronto, Canada',\n",
       " 'My heart is a ghost town!',\n",
       " 'Memphis, TN',\n",
       " 'Contoocook Valley Region of Ne',\n",
       " 'Pittsburgh',\n",
       " 'Los Angeles, CA',\n",
       " 'Hillsville/Lynchburg, VA',\n",
       " 'County Durham, United Kingdom',\n",
       " 'Sochi, KDA, RU',\n",
       " 'Ottawa,Ontario Canada',\n",
       " 'nj',\n",
       " 'Frankfort, KY',\n",
       " 'Ellensburg to Spokane',\n",
       " 'ny',\n",
       " 'Portugal',\n",
       " 'Terre Haute, IN',\n",
       " 'new york',\n",
       " 'Lagos',\n",
       " 'South east of U.K',\n",
       " 'south africa eastern cape',\n",
       " 'Oslo, Norway',\n",
       " 'Lives in London',\n",
       " 'Washington, D.C. ',\n",
       " 'Centurion ',\n",
       " 'Nice places ',\n",
       " 'Cairo, Egypt.',\n",
       " 'i love you zayn',\n",
       " 'Nottingham',\n",
       " 'Ikeja, Nigeria',\n",
       " 'Winnipeg, MB, Canada',\n",
       " 'UK, Republic of Ireland and Australia',\n",
       " 'Somecity, Somerset, MD',\n",
       " 'CLT',\n",
       " 'taken by piper curda',\n",
       " 'manaus',\n",
       " 'Trumann, Arkansas',\n",
       " 'Fredonia,NY',\n",
       " 'hkXfYMhEx',\n",
       " 'I ACCEPT SONG REQUESTS',\n",
       " 'Vancouver',\n",
       " 'Indianapolis, IN',\n",
       " 'Indiana',\n",
       " 'Inverness, Nova Scotia',\n",
       " 'Park Ridge, Illinois',\n",
       " 'london / st catharines ?',\n",
       " 'Yellowknife',\n",
       " 'Live mÌÁs',\n",
       " \"Mpela'zwe \",\n",
       " 'PG County, MD',\n",
       " 'Somalia',\n",
       " '#freegucci',\n",
       " 'Madrid',\n",
       " 'Wyoming, MI (Grand Rapids)',\n",
       " 'wherever-the-fuck washington',\n",
       " 'Everett, WA',\n",
       " ' BC, US, Asia or Europe.',\n",
       " '[ Blonde Bi Fry. ]',\n",
       " 'keli x',\n",
       " 'I Heard #2MBikers',\n",
       " 'Zac Newsome loves me',\n",
       " 'where the wild things are',\n",
       " 'Instagram - @heyimginog ',\n",
       " 'Croydon',\n",
       " 'DC, frequently NYC/San Diego',\n",
       " 'Sumter, SC',\n",
       " 'Inside your mind.',\n",
       " 'w. Nykae ',\n",
       " 'ÌÏT: 40.562796,-75.488849',\n",
       " 'hey Georgia',\n",
       " 'Bon Temps Louisiana',\n",
       " 'Jackson',\n",
       " 'Islamabad',\n",
       " 'The P (South Philly)',\n",
       " 'glasgow',\n",
       " 'Sioux Falls, S.D. ',\n",
       " 'Saint Louis, Missouri',\n",
       " 'North-East Region, Singapore',\n",
       " 'St. Louis, Missouri',\n",
       " 'Garden City, NY',\n",
       " 'CamaquÌ£/Pelotas',\n",
       " 'the azure cloud',\n",
       " 'San Jose, California',\n",
       " 'a botanical garden probably',\n",
       " 'Rogersville, MO',\n",
       " 'Gold Coast, Australia',\n",
       " 'WASHINGTON,DC',\n",
       " 'playing soccer & eating pizza',\n",
       " 'Finland',\n",
       " 'London UK',\n",
       " 'Jersey City, NJ',\n",
       " 'WORDLDWIDE',\n",
       " 'ÌÏT: 33.209923,-87.545328',\n",
       " 'Sligo and Galway, Ireland',\n",
       " 'Sunny South florida ',\n",
       " 'Kawartha Lakes, Ontario, Canad',\n",
       " '#SandraBland',\n",
       " ' The World',\n",
       " 'Morris, IL',\n",
       " \"#WhereverI'mAt\",\n",
       " 'Dudetown',\n",
       " 'Utah, USA',\n",
       " ' New England',\n",
       " 'Jerusalem, Israel',\n",
       " '956',\n",
       " 'The Great State of Maine ',\n",
       " '2005 |-/',\n",
       " 'Yewa zone',\n",
       " 'south of heaven ',\n",
       " 'US',\n",
       " 'Harlem, New York',\n",
       " 'Otsego, MI',\n",
       " 'Conroe, TX',\n",
       " '60th St (SS)',\n",
       " 'Arundel ',\n",
       " 'uk',\n",
       " 'St. Louis',\n",
       " 'I rap to burn shame.',\n",
       " 'Alexandria, VA',\n",
       " 'Asheboro, NC',\n",
       " 'Lurking',\n",
       " 'The Citadel, Oldtown, Westeros',\n",
       " '[Gia.] | #KardashianEmpire',\n",
       " 'Coventry',\n",
       " 'Newton Centre, Massachusetts',\n",
       " ' Bouvet Island',\n",
       " 'miami x dallas ',\n",
       " 'Thane',\n",
       " 'From NY. In Scranton, PA',\n",
       " 'Higher Places',\n",
       " 'Scottsdale, AZ',\n",
       " 'SoDak',\n",
       " '????',\n",
       " 'Texas-USA\\x89ã¢ ?',\n",
       " '\\x89Û¢901\\x89Û¢',\n",
       " 'Broadview Heights, Ohio',\n",
       " 'Tennessee, USA',\n",
       " 'Bangalore, India',\n",
       " 'Over the Moon...',\n",
       " 'Numa casa de old yellow bricks',\n",
       " 'Santa Monica, CA',\n",
       " 'Germany',\n",
       " 'y/e/l',\n",
       " 'West Hollywood, CA',\n",
       " 'Like us on Face ',\n",
       " 'Minneapolis/St. Paul',\n",
       " 'Ikorodu',\n",
       " 'Evanston, IL',\n",
       " 'Above the snake line - #YoNews',\n",
       " 'Novi, MI',\n",
       " 'Frome, Somerset, England',\n",
       " 'Dimapur',\n",
       " 'Here, there and everywhere',\n",
       " 'Michel Delving.',\n",
       " 'South Pasadena, CA',\n",
       " 'Cleveland, OH',\n",
       " 'Dicky Beach',\n",
       " 'Sacramento, California',\n",
       " 'Kingswinford',\n",
       " 'San Diego California 92101',\n",
       " 'Manchester',\n",
       " 'Helsinki',\n",
       " 'Dallas, Tejas',\n",
       " 'Desde Republica Argentina',\n",
       " 'Head Office: United Kingdom',\n",
       " 'Essex, England',\n",
       " 'Prehistoric Earth',\n",
       " '???????????',\n",
       " 'melbourne',\n",
       " 'Wilmington, Delaware',\n",
       " 'Richmond, VA',\n",
       " 'bk. ',\n",
       " 'San Diego, California',\n",
       " 'South Bloomfield, OH',\n",
       " 'Passamaquoddy',\n",
       " 'Clearwater, FL',\n",
       " 'Ontario, Canada. ',\n",
       " '[marvel\\x89Û¢dragon age\\x89Û¢wicdiv]',\n",
       " 'Wildomar, CA',\n",
       " 'IL',\n",
       " 'Mumbai (India)',\n",
       " 'somewhere too cold for me',\n",
       " 'B&B near Alton Towers',\n",
       " 'Alaska, USA',\n",
       " 'My old New England home',\n",
       " 'i got 1/13 menpa replies, omg',\n",
       " \"Eww, I'm not Paul Elam\",\n",
       " 'Rapid City, South Dakota',\n",
       " 'Beit El - Israel',\n",
       " 'austin tx',\n",
       " 'West Lancashire, UK.',\n",
       " 'Abuja,Nigeria',\n",
       " '??????????????????',\n",
       " 'Ewa Beach, HI',\n",
       " 'Shity land of Northern Ireland',\n",
       " 'Atlanta',\n",
       " 'Pocatello, Idaho',\n",
       " 'NAWF SIDE POKING OUT ',\n",
       " 'Palma, Islas Baleares',\n",
       " 'WORLDWIDE-BOSTON',\n",
       " 'Houston, Texas ! ',\n",
       " 'mumbai',\n",
       " 'UK,singer,songwriter,?2 act',\n",
       " 'Dubai, UAE',\n",
       " 'Shady Pines ',\n",
       " 'Scotts Valley, CA',\n",
       " 'Vineyard',\n",
       " '1/3 of the blam squad ',\n",
       " 'Rockland County, NY',\n",
       " 'GREENSBORO,NORTH CAROLINA',\n",
       " '617-BTOWN-BEATDOWN',\n",
       " 'Uruguay / Westeros / Gallifrey',\n",
       " 'Carterville',\n",
       " 'Londonstan',\n",
       " 'Whiterun, Skyrim',\n",
       " 'London ',\n",
       " 'ATL, GA',\n",
       " 'Eddyville, Oregon 97343',\n",
       " 'Chicago',\n",
       " 'Nairobi , Kenya',\n",
       " '?s????ss? a?????',\n",
       " 'Eaubonne, 95, France',\n",
       " '?',\n",
       " 'japon',\n",
       " 'Alberta | Sask. | Montana',\n",
       " 'Calgary, AB, Canada',\n",
       " 'Loughton, Essex, UK',\n",
       " 'TechFish ',\n",
       " 'KSU 2017',\n",
       " 'rural ohio (fuck)',\n",
       " 'Peterborough, Ont.',\n",
       " 'VISIT MY YOUTUBE CHANNEL.',\n",
       " 'the moon',\n",
       " 'Buenos Aires',\n",
       " 'France',\n",
       " 'Columbus',\n",
       " 'Coconut Creek, Florida',\n",
       " 'New Brunswick, NJ',\n",
       " '#????? Libya#',\n",
       " 'Boston, Massachusetts',\n",
       " '5/5 access / rt link please x',\n",
       " \"The Sun's Corona\",\n",
       " 'Greensboro, NC',\n",
       " 'TN',\n",
       " 'Oklahoma City',\n",
       " '??? ???? ??????',\n",
       " 'Ljubljana, Slovenia',\n",
       " 'blackfalds.',\n",
       " 'Pennsylvania, PA',\n",
       " 'America',\n",
       " 'planeta H2o',\n",
       " 'The TARDIS',\n",
       " 'marysville ca ',\n",
       " 'FOLLOWS YOU everywhere you go',\n",
       " 'in my own personal hell (:',\n",
       " 'Worldwide.',\n",
       " 'Melbourne, Victoria',\n",
       " 'Lawrence, KS via Emporia, KS',\n",
       " 'philly ',\n",
       " 'Anaheim',\n",
       " 'Columbus ?? North Carolina',\n",
       " 'Warwick, RI @Dollarocracy also',\n",
       " 'USA, WA',\n",
       " 'Chippenham/Bath, UK',\n",
       " 'U.S.A.   FEMA Region 5',\n",
       " ' Indiana',\n",
       " 'Houston',\n",
       " 'Kuala Lumpur, Malaysia',\n",
       " 'i love the smurfs 2',\n",
       " 'Fountain Valley, CA',\n",
       " 'Hueco Mundo',\n",
       " 'Oregon and Washington',\n",
       " 'Purgatory, USA',\n",
       " 'Puerto Rico',\n",
       " 'Bronx, New York',\n",
       " 'todaysbigstock.com',\n",
       " 'South Korea GMT+9',\n",
       " 'we?it \\x89Û¢ ixwin',\n",
       " 'Avon, OH',\n",
       " 'Jonesboro, AR MO, IOWA USA',\n",
       " 'Ventura',\n",
       " 'Merica!',\n",
       " 'austin, texas',\n",
       " 'MAURITIUS',\n",
       " 'Gander NF',\n",
       " 'San Jose, CA, USA',\n",
       " 'West',\n",
       " 'Buy Give Me My Money ',\n",
       " 'Miami,FL',\n",
       " 'USA, Haiti, Nepal',\n",
       " 'Baton Rouge',\n",
       " 'kediri,,jawa timur',\n",
       " 'ARBAILO',\n",
       " 'Midwest City, OK',\n",
       " 'The Empire/First Order',\n",
       " 'Poconos',\n",
       " 'NYC',\n",
       " 'Tulalip, Washington',\n",
       " 'Oneonta, NY/ Staten Island, NY',\n",
       " 'USA, North Dakota',\n",
       " 'The Desert of the Real',\n",
       " 'Norwich',\n",
       " 'Macclesfield',\n",
       " 'Nairobi, Kenya',\n",
       " 'Windsor,Ontario',\n",
       " 'Orlando/Cocoa Beach, FL',\n",
       " 'Macon, GA',\n",
       " 'Waverly, IA',\n",
       " 'Nashville, TN',\n",
       " 'fluffy cloud',\n",
       " 'WORLD WIDE',\n",
       " 'Suburban Detroit, Michigan',\n",
       " 'Under Santa Barbara Skies',\n",
       " 'sÌ£o luis',\n",
       " 'Madison, GA',\n",
       " 'Cleveland, OH - San Diego, CA',\n",
       " 'Dover, DE',\n",
       " 'St. Joseph, Minnesota',\n",
       " 'Holly, MI',\n",
       " '(RP)',\n",
       " 'Auckland, New Zealand',\n",
       " 'shoujo hell ',\n",
       " 'Bathtub de Bett ',\n",
       " '650/559',\n",
       " 'Goa, India',\n",
       " 'eritrean',\n",
       " 'Griffin :3',\n",
       " 'Budapest, Hungary',\n",
       " 'North Memphis/Global Citizen',\n",
       " 'Principality of Zeron',\n",
       " 'Winnipeg, Manitoba',\n",
       " 'Innsmouth, Mass.',\n",
       " \"Wherever I'm needed\",\n",
       " 'Gotham City,USA',\n",
       " 'Santa Maria, CA',\n",
       " 'Sutton, London UK',\n",
       " 'Sacae Plains',\n",
       " 'Twitter Lockout in progress',\n",
       " 'Moscow',\n",
       " 'Clayton, NC',\n",
       " \"Le Moyne '16\",\n",
       " 'Wolverhampton',\n",
       " 'Amsterdam',\n",
       " 'LONG ISLAND, NY',\n",
       " 'Jersey - C.I',\n",
       " 'BodÌü, Norge',\n",
       " '#EngleWood CHICAGO ',\n",
       " 'Colombia',\n",
       " 'PSN: Pipbois ',\n",
       " 'Jammu | Kashmir | Delhi',\n",
       " \"it's a journey \",\n",
       " 'tripoli international airport',\n",
       " 'Ecuador',\n",
       " 'Tennessee',\n",
       " '?????',\n",
       " 'Loading...',\n",
       " 'Get our App',\n",
       " 'Colorado/WorldWide',\n",
       " 'Milton/Tallahassee',\n",
       " 'Sacramento',\n",
       " 'LIVERPOOL',\n",
       " 'Cherry Creek Denver CO',\n",
       " 'From a torn up town MANCHESTER',\n",
       " 'Atlanta, Georgia USA',\n",
       " 'OES 4th Point. sisSTAR & TI',\n",
       " 'Raleigh Durham, NC',\n",
       " 'Boston, MA',\n",
       " 'Baltimore, MD',\n",
       " 'CHICAGO (312)',\n",
       " 'Birmingham',\n",
       " 'England, Great Britain.',\n",
       " 'God is Love. ',\n",
       " 'Eagle Pass, Texas',\n",
       " 'Melbourne-ish',\n",
       " 'Newcastle Upon Tyne, England',\n",
       " '#MayGodHelpUS',\n",
       " 'Gwersyllt, Wales',\n",
       " 'Menasha, WI',\n",
       " 'Calgary, Canada',\n",
       " 'Piedmont Triad, NC',\n",
       " 'FILM OUT LATE 2015',\n",
       " 'Malaysia',\n",
       " 'Austin TX',\n",
       " 'SE London(heart is by the sea)',\n",
       " 'Charleston S.C.',\n",
       " 'I O W A',\n",
       " 'Victoria, Canada',\n",
       " 'Washington',\n",
       " 'Pocatello, ID',\n",
       " 'WORLDWIDE!',\n",
       " 'Anywhere Safe',\n",
       " 'In your head',\n",
       " 'DC Metro area',\n",
       " 'Paonia, Colorado ',\n",
       " 'Portland, OR',\n",
       " 'Worldwide',\n",
       " 'texas',\n",
       " 'Port Williams NS',\n",
       " 'Florida USA',\n",
       " 'probably not home',\n",
       " 'Canada',\n",
       " 'U.S',\n",
       " 'US-PR',\n",
       " '204, 555 11 Ave. S.W.',\n",
       " 'Bay Area, CA',\n",
       " 'Washington, DC NATIVE',\n",
       " 'right next to you',\n",
       " 'WA State',\n",
       " 'Hong Kong',\n",
       " 'livin in a plastic world',\n",
       " 'pittsboro',\n",
       " 'dorito land',\n",
       " \"In @4SkinChan 's arms\",\n",
       " '11th dimension, los angeles',\n",
       " 'Cimerak - Pangandaran',\n",
       " 'New Zealand',\n",
       " 'North Ferriby, East Yorkshire',\n",
       " 'Charleston, SC',\n",
       " 'mnl',\n",
       " 'Birmingham UK',\n",
       " '3rd Eye Chakra',\n",
       " 'Calgary',\n",
       " 'Campo Grande-MS',\n",
       " 'Mariveles, Bataan',\n",
       " 'Baydestrian',\n",
       " 'Dalston, Hackney',\n",
       " 'Johannesburg, South Africa',\n",
       " 'Las Vegas aka Hell',\n",
       " \"R'lyeh, South Pacific\",\n",
       " 'Smash Manor/Kanto',\n",
       " 'oklahoma',\n",
       " 'Lagos, Nigeria',\n",
       " 'all over the world',\n",
       " 'Buffalo NY',\n",
       " 'Reston, VA, USA',\n",
       " 'Italy',\n",
       " '2,360 miles away',\n",
       " 'Yamaku Academy, Class 3-4',\n",
       " 'moss chamber b',\n",
       " 'wherever the $$$ at',\n",
       " 'NV',\n",
       " '772 Temperance Permenence',\n",
       " 'Jersey',\n",
       " 'Washington State',\n",
       " 'Right next to Compton',\n",
       " '?????? ??? ?????? ????????',\n",
       " 'Downtown Oklahoma City',\n",
       " \"'soooota\",\n",
       " 'My subconscious',\n",
       " 'Hendersonville, NC',\n",
       " '@ ForSL/RP',\n",
       " 'Leicester, England',\n",
       " 'New York, NY ',\n",
       " '  å_ ',\n",
       " 'Canberra, Australian Capital Territory',\n",
       " 'Los Angeles, London, Kent',\n",
       " 'LA - everywhere',\n",
       " 'Earthling (For now!)',\n",
       " 'Quincy MA',\n",
       " 'Oshawa/Toronto',\n",
       " 'Nicola Valley',\n",
       " 'Macon, Georgia',\n",
       " 'houstn',\n",
       " 'Abuja, Nigeria',\n",
       " 'Htx',\n",
       " 'Lincoln, IL',\n",
       " 'EspaÌ±a - Spain - Espagne',\n",
       " 'norway',\n",
       " 'contactSimpleNews@gmail.com',\n",
       " 'All around the world!',\n",
       " 'boston',\n",
       " 'Seattle WA',\n",
       " 'Europe',\n",
       " '  Melbourne, Australia',\n",
       " 'Sylacauga, Alabama',\n",
       " 'Shelby County',\n",
       " 'Antarctica',\n",
       " '#Bummerville otw',\n",
       " '#ODU',\n",
       " 'Host of #MindMoversPodcast',\n",
       " 'Vancouver, Colombie-Britannique',\n",
       " 'Arlington, VA',\n",
       " 'The Internet & NYC',\n",
       " 'San Jose, CA',\n",
       " 'Cape Town',\n",
       " '(Spain)',\n",
       " 'Turkmenistan',\n",
       " 'Murray Hill, New Jersey',\n",
       " 'Halifax, NS, Canada',\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train.location.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "222 unique `keyword` values (including `nan`) out of 7522 non-nan values.<br>\n",
    "One-hot encode it.<br><br>\n",
    "On the other hand, 3342 unique `location` values out of 5080 non-nan values.<br>\n",
    "Seems correct location names and meaningless location names are mixed, but not sure how to separate them.<br>\n",
    "For now, just make it a column of one-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'nan' with 'none'\n",
    "def replaceNAN_none(df, column='keyword'):\n",
    "    col_edit = df[column].values.copy()\n",
    "    col_edit[pd.isnull(col_edit)] = 'none'\n",
    "    col_edit = col_edit.reshape(-1, 1)\n",
    "    return col_edit\n",
    "\n",
    "train_keyword_edit = replaceNAN_none(train)\n",
    "test_keyword_edit = replaceNAN_none(test)\n",
    "\n",
    "# Set up an one-hot encoder.\n",
    "enc = OneHotEncoder(handle_unknown='ignore', dtype=np.int)  # Ignore categories not present in the training data.\n",
    "enc.fit(train_keyword_edit)\n",
    "\n",
    "def replaceCol_onehot(df, column='keyword', encoder=enc):\n",
    "    col_edit = replaceNAN_none(df, column)\n",
    "    cols_onehot = encoder.transform(col_edit).toarray()\n",
    "    \n",
    "    df_new = df.drop(column, axis=1)\n",
    "    df_new = pd.concat([df_new, pd.DataFrame(cols_onehot)], axis=1)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "    \n",
    "train = replaceCol_onehot(train)\n",
    "test = replaceCol_onehot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train.columns.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'location' column and drop the original with 'id' column.\n",
    "train['location_isnull'] = pd.isnull(train.location).astype('int')\n",
    "test['location_isnull'] = pd.isnull(test.location).astype('int')\n",
    "train.drop(['id', 'location'], axis=1, inplace=True)\n",
    "test.drop(['id', 'location'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Our Deeds are the Reason of this #earthquake M...\n",
       "1                  Forest fire near La Ronge Sask. Canada\n",
       "2       All residents asked to 'shelter in place' are ...\n",
       "3       13,000 people receive #wildfires evacuation or...\n",
       "4       Just got sent this photo from Ruby #Alaska as ...\n",
       "                              ...                        \n",
       "7608    Two giant cranes holding a bridge collapse int...\n",
       "7609    @aria_ahrary @TheTawniest The out of control w...\n",
       "7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
       "7611    Police investigating after an e-bike collided ...\n",
       "7612    The Latest: More Homes Razed by Northern Calif...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make bag of words from tweet texts\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def tweet_to_words(text):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text() # Remove HTML tags.\n",
    "    text = re.sub(r\"[,\\.]\", \"\", text)  # Remove ',' and '.'.\n",
    "    text = re.sub(r\"(\\w?)\\d+(\\w?)\", \"\\\\1 thisissomenumber \\\\2\", text)  # Replace all number words with a token text.\n",
    "    text = re.sub(r\"\\d+\", \"thisissomenumber\", text)  # Replace all number words with a token text.\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    words = ' '.join(words)  # Make it back into a sentence\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deed reason earthquak may allah forgiv us'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_to_words(train.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_train = train.text.apply(tweet_to_words)\n",
    "words_test = test.text.apply(tweet_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            deed reason earthquak may allah forgiv us\n",
       "1                 forest fire near la rong sask canada\n",
       "2    resid ask shelter place notifi offic evacu she...\n",
       "3    thisissomenumb thisissomenumb peopl receiv wil...\n",
       "4    got sent photo rubi alaska smoke wildfir pour ...\n",
       "5    rockyfir updat california hwi thisissomenumb t...\n",
       "6    flood disast heavi rain caus flash flood stree...\n",
       "7                               top hill see fire wood\n",
       "8               emerg evacu happen build across street\n",
       "9                             afraid tornado come area\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning:\n",
      "\n",
      "sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "cache_dir = 'cache/'\n",
    "cache_file = 'bagOfWords.pkl'\n",
    "\n",
    "if not os.path.exists('cache'):\n",
    "    subprocess.check_call('mkdir cache', shell=True)\n",
    "\n",
    "def extract_BoW_features(\n",
    "    words_train, words_test, \n",
    "    vocabulary_size=5000,\n",
    "    cache_dir=cache_dir, cache_file=cache_file\n",
    "):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        print(\"Make bag of words from scratch.\")\n",
    "        # Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features=vocabulary_size)\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        print(\"Load bag of words from cache.\")\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read features from cache file: bagOfWords.pkl\n",
      "Load bag of words from cache.\n"
     ]
    }
   ],
   "source": [
    "bow_train, bow_test, vocabulary = extract_BoW_features(words_train, words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target  0  1  2  3  4  5  6  7  8  ...  4990  4991  4992  4993  4994  4995  \\\n",
      "0       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
      "1       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
      "2       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
      "3       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
      "4       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
      "\n",
      "   4996  4997  4998  4999  \n",
      "0     0     0     0     0  \n",
      "1     0     0     0     0  \n",
      "2     0     0     0     0  \n",
      "3     0     0     0     0  \n",
      "4     0     0     0     0  \n",
      "\n",
      "[5 rows x 5224 columns]\n",
      "   0     1     2     3     4     5     6     7     8     9     ...  4990  \\\n",
      "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "3     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "4     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "\n",
      "   4991  4992  4993  4994  4995  4996  4997  4998  4999  \n",
      "0     0     0     0     0     0     0     0     0     0  \n",
      "1     0     0     0     0     0     0     0     0     0  \n",
      "2     0     0     0     0     0     0     0     0     0  \n",
      "3     0     0     0     0     0     0     0     0     0  \n",
      "4     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 5223 columns]\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([train, pd.DataFrame(bow_train)], axis=1)\n",
    "test = pd.concat([test, pd.DataFrame(bow_test)], axis=1)\n",
    "train.drop('text', axis=1, inplace=True)\n",
    "test.drop('text', axis=1, inplace=True)\n",
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>4990</th>\n",
       "      <th>4991</th>\n",
       "      <th>4992</th>\n",
       "      <th>4993</th>\n",
       "      <th>4994</th>\n",
       "      <th>4995</th>\n",
       "      <th>4996</th>\n",
       "      <th>4997</th>\n",
       "      <th>4998</th>\n",
       "      <th>4999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5224 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  0  1  2  3  4  5  6  7  8  ...  4990  4991  4992  4993  4994  4995  \\\n",
       "0       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "1       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "2       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "3       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "4       1  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0     0   \n",
       "\n",
       "   4996  4997  4998  4999  \n",
       "0     0     0     0     0  \n",
       "1     0     0     0     0  \n",
       "2     0     0     0     0  \n",
       "3     0     0     0     0  \n",
       "4     0     0     0     0  \n",
       "\n",
       "[5 rows x 5224 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the target values the first column.\n",
    "train_y = train['target']\n",
    "train_x = train.drop('target', axis=1)\n",
    "train = pd.concat([train_y, train_x], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('processed_data'):\n",
    "    subprocess.check_call('mkdir processed_data', shell=True)\n",
    "\n",
    "train.to_csv(\"processed_data/train_processed.csv\", header=False, index=False)\n",
    "test.to_csv(\"processed_data/test_processed.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing\n",
      "    train shape: (7613, 5224)\n",
      "    test shape: (3263, 5223)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"After preprocessing\\n\\\n",
    "    train shape: {}\\n\\\n",
    "    test shape: {}\".format(\n",
    "        train.shape,\n",
    "        test.shape\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
