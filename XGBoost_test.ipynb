{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from time import localtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker parameters\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'kaggle/tweetSentiment'  # Prefix should not tontain '/' at the end!\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "working_dir = '/home/ec2-user/SageMaker/kaggle_data'\n",
    "data_dir = os.path.join(working_dir, 'processed_data/')\n",
    "output_dir = os.path.join(working_dir, 'output/')\n",
    "if not os.path.exists(output_dir):\n",
    "    subprocess.check_call('mkdir {}'.format(output_dir), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data already present in S3.\n"
     ]
    }
   ],
   "source": [
    "# data upload to S3\n",
    "s3_object_dict = s3.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=prefix\n",
    ")\n",
    "\n",
    "# Fetch filenames in the data directory.\n",
    "local_data_list = os.listdir(data_dir)\n",
    "\n",
    "# Combine the name with the S3 bucket prefix.\n",
    "local_data_list = [os.path.join(prefix, f) for f in local_data_list]\n",
    "\n",
    "# Upload the data if they are not present in S3.\n",
    "try:\n",
    "    s3_object_list = [content['Key'] for content in s3_object_dict['Contents']]\n",
    "    if set(local_data_list).intersection(s3_object_list) == set(local_data_list):\n",
    "        test_location = os.path.join('s3://', bucket, prefix, 'test_processed.csv')        \n",
    "        print(\"input_data already present in S3.\")\n",
    "        \n",
    "    else:\n",
    "        # Split the training data to train and velidation data.\n",
    "        train = pd.read_csv(os.path.join(data_dir, 'train_processed.csv'), header=None)\n",
    "        train, validation = train_test_split(\n",
    "            train, \n",
    "            stratify=train[0].values,\n",
    "            test_size=0.25,\n",
    "            random_state=0\n",
    "        )\n",
    "        \n",
    "        # Save them locally.\n",
    "        train.to_csv(os.path.join(data_dir, 'train_processed_split.csv'), header=None, index=None)\n",
    "        validation.to_csv(os.path.join(data_dir, 'validation.csv'), header=None, index=None)\n",
    "        \n",
    "        train = None,\n",
    "        validation = None\n",
    "        \n",
    "        # Upload the data to S3.\n",
    "        test_location = session.upload_data(\n",
    "            path=os.path.join(data_dir, 'test_processed.csv'), \n",
    "            bucket=bucket,\n",
    "            key_prefix=prefix\n",
    "        )\n",
    "        \n",
    "        print(\"train and validation data uploaded to S3.\")\n",
    "        \n",
    "except KeyError:  # if nothing exists in the S3 bucket.\n",
    "    # Upload the data to S3.\n",
    "    test_location = session.upload_data(\n",
    "        path=os.path.join(data_dir, 'test_processed.csv'), \n",
    "        bucket=bucket,\n",
    "        key_prefix=prefix\n",
    "    )\n",
    "    \n",
    "    print(\"train and validation data uploaded to S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning-job2020-05-30-19-45-31\n"
     ]
    }
   ],
   "source": [
    "with open('tuning_job_name.txt', 'r') as f:\n",
    "    tuning_job_name = f.read().split()[0]\n",
    "print(tuning_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_info = session.sagemaker_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)\n",
    "best_training_job_name = tuning_job_info['BestTrainingJob']['TrainingJobName']\n",
    "training_job_info = session.sagemaker_client.describe_training_job(TrainingJobName=best_training_job_name)\n",
    "\n",
    "model_artifacts = training_job_info['ModelArtifacts']['S3ModelArtifacts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(session.boto_region_name, 'xgboost', '0.90-1')\n",
    "\n",
    "# Just like when we created a training job, the model name must be unique\n",
    "model_name = best_training_job_name + \"-model\"\n",
    "\n",
    "# We also need to tell SageMaker which container should be used for inference and where it should\n",
    "# retrieve the model artifacts from. In our case, the xgboost container that we used for training\n",
    "# can also be used for inference.\n",
    "primary_container = {\n",
    "    \"Image\": container,\n",
    "    \"ModelDataUrl\": model_artifacts\n",
    "}\n",
    "\n",
    "# And lastly we construct the SageMaker model\n",
    "model_info = session.sagemaker_client.create_model(\n",
    "                                ModelName = model_name,\n",
    "                                ExecutionRoleArn = role,\n",
    "                                PrimaryContainer = primary_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like in each of the previous steps, we need to make sure to name our job and the name should be unique.\n",
    "transform_job_name = 'boston-xgboost-batch-transform-' + strftime(\"%Y-%m-%d-%H-%M-%S\", localtime())\n",
    "\n",
    "# Now we construct the data structure which will describe the batch transform job.\n",
    "transform_request = {\n",
    "    \"TransformJobName\": transform_job_name,\n",
    "    \"ModelName\": model_name,\n",
    "    \"MaxConcurrentTransforms\": 1,\n",
    "    \"MaxPayloadInMB\": 6,\n",
    "    \"BatchStrategy\": \"MultiRecord\",\n",
    "    \"TransformOutput\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}/batchTransform/\".format(bucket, prefix)\n",
    "    },\n",
    "    \"TransformInput\": {\n",
    "        \"ContentType\": \"text/csv\",\n",
    "        \"SplitType\": \"Line\",\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": test_location,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"TransformResources\": {\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"InstanceCount\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................................!\n"
     ]
    }
   ],
   "source": [
    "transform_response = session.sagemaker_client.create_transform_job(**transform_request)\n",
    "transform_desc = session.wait_for_transform_job(transform_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_output = \"s3://{}/{}/batchTransform/test_processed.csv.out\".format(bucket, prefix)\n",
    "subprocess.check_call(\n",
    "    'aws s3 cp {} {}'.format(\n",
    "        transform_output, os.path.join(output_dir, 'test_processed.csv.out')\n",
    "    ),\n",
    "    shell=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "id = test.id\n",
    "pred = pd.read_csv(os.path.join(output_dir, 'test_processed.csv.out'), header=None)\n",
    "out = pd.concat([id, pred.round().astype(int)], axis=1)\n",
    "out.columns = ['id', 'target']\n",
    "\n",
    "out.to_csv(os.path.join(output_dir, 'out.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.check_call('aws s3 cp {} s3://{}/{}/out.csv'.format(os.path.join(output_dir, 'out.csv'), bucket, prefix), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
